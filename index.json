[{"authors":null,"categories":null,"content":"Katharine Hyatt is a postdoc, working in the Center for Computational Quantum Physics at the Flatiron Institute, a division of the Simons Foundation. Her research focuses on developing new numerical methods to understand 2D correlated electronic systems, and finding interesting applications in condensed matter physics for these methods. Tensor networks play an important role in this research, along with methods like exact diagonalization and quantum Monte Carlo. She was previously a graduate student at the University of California, Santa Barbara, where she received her PhD in physics in June 2018. Her undergraduate study was completed at the University of Waterloo, from which she holds an Honours BSc in Mathematical Physics. She also moonlights as a sometime Julia language and package developer.\n","date":1583121631,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1577731519,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kshyatt.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Katharine Hyatt is a postdoc, working in the Center for Computational Quantum Physics at the Flatiron Institute, a division of the Simons Foundation. Her research focuses on developing new numerical methods to understand 2D correlated electronic systems, and finding interesting applications in condensed matter physics for these methods. Tensor networks play an important role in this research, along with methods like exact diagonalization and quantum Monte Carlo. She was previously a graduate student at the University of California, Santa Barbara, where she received her PhD in physics in June 2018.","tags":null,"title":"Katharine Hyatt","type":"authors"},{"authors":["Katharine Hyatt"],"categories":null,"content":"","date":1583121631,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"969957a23363c2f265eb3faefbb9beaa","permalink":"https://kshyatt.github.io/talk/aps2020/","publishdate":"2019-10-18T18:00:31-04:00","relpermalink":"/talk/aps2020/","section":"talk","summary":"","tags":[],"title":"March Meeting 2020","type":"talk"},{"authors":["Katharine Hyatt"],"categories":[],"content":"Last year at JuliaCon, Matt Fishman and I gave a talk about our ongoing effort to port the ITensor code from C++ to Julia. At the time, I mentioned that we had begun trying to integrate a GPU-based contraction backend and were looking forward to some significant speedups.\nWhat is ITensor To understand why we thought this would be fruitful, and some performance traps we could already anticipate before writing a single line of GPU code, you have to understand a bit about the problem we're trying to solve. Tensor network algorithms are a very active area of research at the intersection of condensed matter physics, high energy physics, quantum information, and computer science. DMRG, which is the most successful numerical method for condensed matter systems in 1D, wasn't originally formulated as a tensor network algorithm, but \u0026ldquo;tensor network speak\u0026rdquo; turns out to be a natural language with which to discuss DMRG and its descendants. (If you're not sure what it's meant to solve, imagine a long chain of quantum objects, each with a fretful relationship with their neibhours. Will they overcome their differences and work together to form a magnet, or continue arguing with each other and fail to come to a consensus? DMRG is an efficient way to answer this question.) All this is to say that there's this class of algorithms physicists (and increasingly computer scientists) are interested in and they work well.\nThe driving idea of a tensor network algorithm is to take some high-dimensional optimization problem (solving for groundstates of quantum many body systems is an example of this, since the full dimension of the system grows exponentially in the number of constituent particles) and compress it down to a much lower dimensional problem while retaining most of the important features. We do this by taking a d^N length vector, where d is the number of degrees of freedom of each constituent, and N is the number of constituents, and breaking it down into a set of multidimensionsal tensors, the number of which hopefully scales like N or at least much less than exponentially.\nIf you've studied linear algebra before, you've seen some simple examples of tensors: scalars, vectors, and matrices. We say a scalar is a 0-rank tensor, a vector a 1-rank tensor (since it has one index), a matrix a 2-rank tensor (since it has two indices), and then there are higher rank tensors, with three or more indices. When we multiply two matrices A_ij and B_jk together to get C_ik, we're performing a \u0026ldquo;tensor contratction\u0026rdquo;. Similarly, if we had high-rank tensors A_ijkl and B_lmin, we could contract them to get C_jkmn.\nMost tensor network algorithms are based on performing this decomposition and then iteratively improving it towards a target vector. Usually in physics that target is a physical state, but tensor networks have also been used for machine learning tasks and can represent quantum error correcting codes as well.\nITensor is a C++ package dedicated to providing both high level algorithms using tensor networks, like DMRG, and the low-level building blocks to create your own. ITensor makes it easy to create tensors out of indices, perform linear algebra factorizations (such as QR or SVD) on them, without forcing the user to worry about index ordering. You can read the ITensor tutorials for more information or watch our talk.\nWhy use the GPU It seems pretty reasonable that you could expect a speedup for many tensor network algorithms by using a GPU. By permuting indices, it's possible to reduce all contraction operations to matrix-matrix or matrix-vector multiplications, at which the GPU excels. Most tensor network algorithms have runtimes dominated by such contractions or by SVD. However, there are some performance gotchas we always need to consider when using the GPU:\n The device has comparatiely low memory. The most expensive cards have 32GB of onboard RAM, which is a lot, but many state-of-the-art DMRG calculations require over a terabyte of RAM or checkpointing by writing intermediate information to disk. There's high latency and low bandwidth for memory transfers. If we absolutely have to copy memory from the host CPU to the device, we should try to do it all in one big blob, and not in many small chunks. Although the GPU can overlap computations and memory transfers writing code to handle this can be a bit complex. The performance for single precision floats is much better than for double precision. Although we'll probably see a perfomance boost for doubles, it won't be as dramatic as for single precision Float32.  In addition, there is a danger in the most naive approach to handling tensor contractions, which is to just permute all involved tensors into the index layout necessary to write the operation as a matrix-matrix multiplication, and then sit back and call GEMM. Although in many cases this will work quite well, especially if the permutation doesn't involve many indices, there are plenty of bad cases where a great deal of time could be spent alllocating destination arrays and permuting source arrays into them. The risk of this increases with the average number of indices on each tensor (since there are more \u0026ldquo;bad\u0026rdquo; permutations available).\nFor these reasons, we weren't sure if the GPU would be a good choice for my current reseach project. You can read the physics details here. We have a C++ implementation of this code which is CPU only and runs about 5000 lines. One of my goals with the project was to eventually open source the code in the hope that others might find it useful or improve it. However, C++, despite being a great langauge, can be intimidating to many people. I have some experience writing C code that uses CUDA which, despite the powerful API and really granular control over the device the programmer is provided, can also be intimidating and require you to keep a lot of balls in the air while you're writing the code. But the C++ solution, stuck on a single node as it was, with all parallelism coming from CPU BLAS spread over 28 cores, was taking up to a week to run to get a decent picture of the converged result. This was pretty frustrating from a development perspective because it meant the debug cycle of \u0026ldquo;something's wrong\u0026rdquo; - \u0026ldquo;OK, think I found it\u0026rdquo; - \u0026ldquo;is this a fix?\u0026rdquo; - \u0026ldquo;nope, something's still wrong\u0026rdquo; had to take place over multiple days.\nSince Miles (the original author of ITensor) and Matt were already thinking of rewriting ITensor in Julia (see our talk for the motivations for this decision), I decided I would try to help and maybe try to add some GPU support to the new package. Many tensor network algorithms, not only this one, are dominated by tensor-tensor contractions as mentioned above. And since I had already had some experience working with Julia's GPU programming/wrapping infrastructre in CuArrays.jl, I thought it wouldn't be so hard to integrate a GPU based tensor operations backend to ITensors.jl. (In fact, we sometimes want to add or subtract tensors, not just contract them.)\nOur first approach, and one I don't have benchmarks for, was the naive method described above - just permute everything and call CUBLAS's general matrix-matrix multiplication routine. In general, handling GPU memory with CuArrays.jl was very easy. An ITensor is essentially an opaque Vector with some indices along for the ride, which tell you in what order to index elements of the Vector. It's analogous to CartesianArray for those who have used Julia's multidimensional array support. Since our algorithms usually require us to somehow achieve a contraction, QR decomposition, and addition, we thought treating the ITensor storage as essentially a blob you can permute and give to multiplication API calls would be enough. Usually in these algorithms you're not often accessing or manipulating single elements or slices of the ITensor (although this is possible to do and easy in both the C++ and Julia versions), just the tensors themselves.\nHere's the sum total of the code I needed to get a barebones cuITensor that you could move on and off the GPU:\nfunction cuITensor(::Type{T},inds::IndexSet) where {T\u0026lt;:Number} return ITensor(Dense{float(T)}(CuArrays.zeros(float(T),dim(inds))), inds) end cuITensor(::Type{T},inds::Index...) where {T\u0026lt;:Number} = ITensor(T,IndexSet(inds...)) cuITensor(is::IndexSet) = cuITensor(Float64,is) cuITensor(inds::Index...) = cuITensor(IndexSet(inds...)) cuITensor() = ITensor() function cuITensor(x::S, inds::IndexSet{N}) where {S\u0026lt;:Number, N} dat = CuVector{float(S)}(undef, dim(inds)) fill!(dat, float(x)) ITensor(Dense{S}(dat), inds) end cuITensor(x::S, inds::Index...) where {S\u0026lt;:Number} = cuITensor(x,IndexSet(inds...)) function cuITensor(A::Array{S},inds::IndexSet) where {S\u0026lt;:Number} return ITensor(Dense(CuArray{S}(A)), inds) end function cuITensor(A::CuArray{S},inds::IndexSet) where {S\u0026lt;:Number} return ITensor(Dense(A), inds) end cuITensor(A::Array{S}, inds::Index...) where {S\u0026lt;:Number} = cuITensor(A,IndexSet(inds...)) cuITensor(A::CuArray{S}, inds::Index...) where {S\u0026lt;:Number} = cuITensor(A,IndexSet(inds...)) cuITensor(A::ITensor) = cuITensor(A.store.data,A.inds) function Base.collect(A::ITensor) typeof(A.store.data) \u0026lt;: CuArray \u0026amp;\u0026amp; return ITensor(collect(A.store), A.inds) return A end Mostly, this handles different ways of providing the indices, and a few options for the input data type. I assumed that if you called the cuITensor constructor but gave it an input CPU array, you probably wanted that array transferred to the GPU. That's the easy part. Adding support for some other operations, like QR decomposition or eigensolving, wasn't much harder:\nfunction eigenHermitian(T::CuDenseTensor{ElT,2,IndsT}; kwargs...) where {ElT,IndsT} maxdim::Int = get(kwargs,:maxdim,minimum(dims(T))) mindim::Int = get(kwargs,:mindim,1) cutoff::Float64 = get(kwargs,:cutoff,0.0) absoluteCutoff::Bool = get(kwargs,:absoluteCutoff,false) doRelCutoff::Bool = get(kwargs,:doRelCutoff,true) local DM, UM if ElT \u0026lt;: Complex DM, UM = CUSOLVER.heevd!(\u0026#39;V\u0026#39;, \u0026#39;U\u0026#39;, matrix(T)) else DM, UM = CUSOLVER.syevd!(\u0026#39;V\u0026#39;, \u0026#39;U\u0026#39;, matrix(T)) end DM_ = reverse(DM) truncerr, docut, DM = truncate!(DM_;maxdim=maxdim, cutoff=cutoff, absoluteCutoff=absoluteCutoff, doRelCutoff=doRelCutoff) dD = length(DM) dV = reverse(UM, dims=2) if dD \u0026lt; size(dV,2) dV = CuMatrix(dV[:,1:dD]) end # Make the new indices to go onto U and V u = eltype(IndsT)(dD) v = eltype(IndsT)(dD) Uinds = IndsT((ind(T,1),u)) Dinds = IndsT((u,v)) dV_ = CuArrays.zeros(ElT, length(dV)) copyto!(dV_, vec(dV)) U = Tensor(Dense(dV_),Uinds) D = Tensor(Diag(real.(DM)),Dinds) return U,D end This weird looking method of getting dV into dV_ is necessary because of the way CuArrays.jl deals with reshapes. As we'll see later it doesn't seem to impact performance too much. But of course, the big problem we wanted to solve was contractions. Because the CPU code also works by performing the permutation and calling GEMM, it was relatively easy to pirate that over to the GPU:\nfunction contract!(C::CuArray{T}, p::CProps, A::CuArray{T}, B::CuArray{T}, α::Tα=1.0, β::Tβ=0.0) where {T,Tα\u0026lt;:Number,Tβ\u0026lt;:Number} # bunch of code to find permutations and permute α and β goes here! CUBLAS.gemm_wrapper!(cref, tA,tB,aref,bref,promote_type(T,Tα)(α),promote_type(T,Tβ)(β)) if p.permuteC permutedims!(C,reshape(cref,p.newCrange...),p.PC) end return end The design of ITensors.jl specifies that the ITensor type itself is not specialiazed on its storage type, so that from the user's point of view, they have a tensor-like object contracting with another tensor-like object, and the developers can worry about how to multiply a diagonal-like rank-6 tensor by a sparse rank-4 tensor. This makes it easier for users to implement the algorithms they need to do their research in, and it's one of the library's strengths. All that was needed to allow an ITensor with GPU-backed storage to play nicely with an ITensor with CPU-backed storage was few lines of edge case handling:\nfunction contract!!(R::CuDenseTensor{\u0026lt;:Number,NR}, labelsR::NTuple{NR}, T1::DenseTensor{\u0026lt;:Number,N1}, labelsT1::NTuple{N1}, T2::CuDenseTensor{\u0026lt;:Number,N2}, labelsT2::NTuple{N2}) where {NR,N1,N2} return contract!!(R, CuDenseTensor(cu(store(T1)), inds(T1)), labelsT1, T2, labelsT2) end function contract!!(R::CuDenseTensor{\u0026lt;:Number,NR}, labelsR::NTuple{NR}, T1::CuDenseTensor{\u0026lt;:Number,N1}, labelsT1::NTuple{N1}, T2::DenseTensor{\u0026lt;:Number,N2}, labelsT2::NTuple{N2}) where {NR,N1,N2} return contract!!(R, T1, labelsT1, CuDenseTensor(cu(store(T2)), inds(T2)), labelsT2) end I chose to copy the CPU storage to the device before the addition or contraction, hoping that this would occur rarely and that the performance gain in the main operation would offset the memory transfer time. Ideally this situation should never occur: we absolutely want to minimize memory transfers. However, if a user makes a mistake and forgets a cuITensor(A), their code won't error out. In fact, in the latest version of ITensorsGPU.jl this dubious feature is disallowed, since in my code it was always the result of forgetting to initialize something on the GPU which should have been.\nThis was enough to get the barebones GPU support working. But I was still worried about the issue with the permutations, especially because the week-long simulations are those which are most memory intensive, and I was worried about running out of space on the device. Could there be a better solution?\nCUTENSOR and the story of how AI made my labour useless Around this time we became aware of CUTENSOR, an NVIDIA library designed exactly for our use case: adding and contracting high rank tensors with indices in arbitrary order. However, this library was, of course, written in C. Luckily Julia makes it pretty easy to wrap C APIs, and we got started doing so in this epic PR. During the process of getting these wrappers into a state fit for a public facing library, Tim created some very nice scripts which automate the process of creating Julia wrappers for C functions, automating away many hours of labour I had performed years ago to get the sparse linear algebra and solver libraries working. Sic transit gloria mundi, I guess (generating these wrappers was not a glorious process). Now it will be easy for us to integrate changes to the CUTENSOR API over time as more features are added.\nCUTENSOR's internals handle matching up elements for products and sums as part of the contraction process, so the permutations that ITensors.jl performs for a CPU-based ITensor are unnecessary. By overriding a few functions we're able to call the correct internal routines which feed through to CUTENSOR:\nfunction Base.:+(B::CuDenseTensor, A::CuDenseTensor) opC = CUTENSOR.CUTENSOR_OP_IDENTITY opA = CUTENSOR.CUTENSOR_OP_IDENTITY opAC = CUTENSOR.CUTENSOR_OP_ADD Ais = inds(A) Bis = inds(B) ind_dict = Vector{Index}() for (idx, i) in enumerate(inds(A)) push!(ind_dict, i) end Adata = data(store(A)) Bdata = data(store(B)) reshapeBdata = reshape(Bdata,dims(Bis)) reshapeAdata = reshape(Adata,dims(Ais)) # probably a silly way to handle this, but it worked ctainds = zeros(Int, length(Ais)) ctbinds = zeros(Int, length(Bis)) for (ii, ia) in enumerate(Ais) ctainds[ii] = findfirst(x-\u0026gt;x==ia, ind_dict) end for (ii, ib) in enumerate(Bis) ctbinds[ii] = findfirst(x-\u0026gt;x==ib, ind_dict) end ctcinds = copy(ctbinds) C = CuArrays.zeros(eltype(Bdata), dims(Bis)) CUTENSOR.elementwiseBinary!(one(eltype(Adata)), reshapeAdata, ctainds, opA, one(eltype(Bdata)), reshapeBdata, ctbinds, opC, C, ctcinds, opAC) copyto!(data(store(B)), vec(C)) return B end Once these wrappers and their tests were merged into CuArrays.jl, I set about changing up how we were calling the contraction functions on the ITensors.jl side. Dealing with optional dependencies, as CuArrays.jl would have been for ITensors.jl, is still kind of a pain in Julia, so I made a new package, ITensorsGPU.jl, to hold all the CUDA-related logic. What's nice is that from the end-user's perspective, they just have copy the tensors to the GPU at the start of the simulation and afterwards everything works mostly seamlessly \u0026ndash; they don't have to concern themselves with index orders or anything. It frees the user to focus more on high-level algorithm design.\nExtirpating memory copies Copying memory back and forth from the device is extremely slow, and the code will perform best if we can eliminate as many as possible. One way to see how much time the device is spending on them is using NVIDIA's nvprof tool. Working with the cluster means I usually do most of my development over SSH, so I used command line mode, which is really easy:\nnvprof ~/software/julia/julia prof_run.jl This generates some output like:\n==1386746== Profiling application: julia prof_run.jl ==1386746== Profiling result: Type Time(%) Time Calls Avg Min Max Name GPU activities: 14.02% 12.1800s 80792 150.76us 102.69us 408.00us void sytd2_upper_cta\u0026lt;double, double, int=5\u0026gt;(int, double*, int, double*, double*, double*) 7.44% 6.46842s 5077903 1.2730us 1.1190us 54.367us [CUDA memcpy HtoD] 6.02% 5.22885s 4430677 1.1800us 959ns 7.2640us ptxcall_anonymous19_1 5.06% 4.39301s 178968 24.546us 8.4480us 81.855us void cutensor_internal_namespace::tensor_contraction_kernel\u0026lt;cutensor_internal_namespace::tc_config_t\u0026lt;int=8, int=4, int=64, int=64, int=1, int=32, int=32, int=1, int=8, int=4, int=8, int=1, int=1, int=1, int=1, int=4, bool=1, bool=0, bool=0, bool=1, bool=0, cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool=0\u0026gt;, double, double, double, double\u0026gt;(cutensor_internal_namespace::tc_params_t, int=1, int=4 const *, int=64 const *, cutensor_internal_namespace::tc_params_t, int=64*) 4.97% 4.31812s 346589 12.458us 5.4400us 29.088us void cutensor_internal_namespace::tensor_contraction_kernel\u0026lt;cutensor_internal_namespace::tc_config_t\u0026lt;int=8, int=4, int=64, int=64, int=1, int=16, int=16, int=1, int=8, int=4, int=4, int=1, int=1, int=1, int=1, int=4, bool=1, bool=1, bool=0, bool=0, bool=0, cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool=0\u0026gt;, double, double, double, double\u0026gt;(cutensor_internal_namespace::tc_params_t, int=1, int=4 const *, int=64 const *, cutensor_internal_namespace::tc_params_t, int=64*) 4.96% 4.31109s 2397744 1.7970us 1.7270us 6.8160us ptxcall_setindex_kernel__26 4.18% 3.63043s 228988 15.854us 5.8560us 28.672us void cutensor_internal_namespace::tensor_contraction_kernel\u0026lt;cutensor_internal_namespace::tc_config_t\u0026lt;int=8, int=4, int=64, int=64, int=1, int=16, int=16, int=1, int=8, int=4, int=4, int=1, int=1, int=1, int=1, int=4, bool=1, bool=0, bool=0, bool=0, bool=0, cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool=0\u0026gt;, double, double, double, double\u0026gt;(cutensor_internal_namespace::tc_params_t, int=1, int=4 const *, int=64 const *, cutensor_internal_namespace::tc_params_t, int=64*) 3.41% 2.96655s 2162240 1.3710us 1.2470us 3.4880us [CUDA memcpy DtoH] 2.77% 2.40280s 1827148 1.3150us 1.0870us 7.3600us ptxcall_anonymous19_14 2.70% 2.34219s 1327280 1.7640us 1.5360us 6.5920us ptxcall_setindex_kernel__15 5424,21 96% ... You can see the memcpyHtoD there, and it's taking up a lot of time! By carefully going through and creating ITensors with the appropriate storage type in internal routines, like so:\n# again probably a nicer way to do this is_gpu = all([data(store(A[i,j])) isa CuArray for i in 1:Ny, j in 1:Nx) N = spinI(findindex(A[row, col], \u0026#34;Site\u0026#34;); is_gpu=is_gpu) function spinI(s::Index; is_gpu::Bool=false)::ITensor I_data = is_gpu ? CuArrays.zeros(Float64, dim(s)*dim(s)) : zeros(Float64, dim(s), dim(s)) idi = diagind(reshape(I_data, dim(s), dim(s)), 0) I_data[idi] = is_gpu ? CuArrays.ones(Float64, dim(s)) : ones(Float64, dim(s)) I = is_gpu ? cuITensor( I_data, IndexSet(s, s\u0026#39;) ) : ITensor(vec(I_data), IndexSet(s, s\u0026#39;)) return I end it's possible to dramatically cut down on this, getting a final profiling report of\n==3303697== Profiling application: julia prof_run.jl ==3303697== Profiling result: Type Time(%) Time Calls Avg Min Max Name GPU activities: 13.78% 19.2803s 343307 56.160us 15.328us 7.2638ms cutensor_internal_namespace::contraction_kernel(cutensor_internal_namespace::KernelParam_double_iden_1_2_false_false_double_iden_1_2_false_false_double_1_double_double_tb_128_128_8_simt_sm50_256) 13.47% 18.8464s 440326 42.800us 15.071us 653.75us cutensor_internal_namespace::contraction_kernel(cutensor_internal_namespace::KernelParam_double_iden_1_2_false_false_double_iden_1_2_true_false_double_1_double_double_tb_128_128_8_simt_sm50_256) 9.70% 13.5765s 262876 51.646us 15.360us 197.02us cutensor_internal_namespace::contraction_kernel(cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_true_false_double_1_double_double_tb_128_128_8_simt_sm50_256) 8.90% 12.4562s 114666 108.63us 15.648us 4.7480ms cutensor_internal_namespace::contraction_kernel(cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_false_false_double_1_double_double_tb_128_128_8_simt_sm50_256) 7.96% 11.1327s 305932 36.389us 22.559us 76.831us void gesvdbj_batch_32x16\u0026lt;double, double\u0026gt;(int, int const *, int const *, int const *, int, double const *, int, double, double*, double*, int*, double, int, double) 5.15% 7.21024s 2439080 2.9560us 2.0480us 6.8470us void ormtr_gerc\u0026lt;double, int=5, int=3, int=1\u0026gt;(int, double const *, int, int, double*, unsigned long, double const *, int, double const *) 3.62% 5.06010s 1663200 3.0420us 1.6630us 5.8240us void sytd2_symv_upper\u0026lt;double, int=4\u0026gt;(int, double const *, double const *, unsigned long, double const *, double*) 3.43% 4.80411s 2439080 1.9690us 1.5680us 5.4390us void ormtr_gemv_c\u0026lt;double, int=4\u0026gt;(int, int, double const *, unsigned long, double const *, int, double*) 2.84% 3.97216s 1636800 2.4260us 2.1120us 5.6000us void larfg_kernel_fast\u0026lt;double, double, int=6\u0026gt;(int, double*, double*, int, double*) 2.57% 3.59984s 2123225 1.6950us 864ns 90.111us [CUDA memcpy DtoD] 2.37% 3.31885s 1663200 1.9950us 1.1520us 4.5760us void sytd2_her2k_kernel\u0026lt;double, int=8, int=4\u0026gt;(int, double*, unsigned long, double const *, int, double const *) 2.25% 3.14692s 611864 5.1430us 4.6390us 12.704us void svd_column_rotate_batch_32x16\u0026lt;double, int=5, int=3\u0026gt;(int, int const *, int const *, int, double*, int, double*, int, double const *, int*) 2.17% 3.03782s 54611 55.626us 3.1990us 315.74us void cutensor_internal_namespace::reduction_kernel\u0026lt;bool=1, int=2, int=6, int=256, int=1, int=256, bool=1, cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, double, double, double, double, double\u0026gt;(double, double const *, double const *, cutensor_internal_namespace::reduction_kernel\u0026lt;bool=1, int=2, int=6, int=256, int=1, int=256, bool=1, cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, double, double, double, double, double\u0026gt;, double const *, double const **, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensor_internal_namespace::reduction_params_t) 2.11% 2.95838s 29720 99.541us 30.367us 1.3328ms cutensor_internal_namespace::contraction_kernel(cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_false_true_double_1_double_double_tb_128_128_8_simt_sm50_256) 2.08% 2.91405s 1663200 1.7520us 1.5040us 4.5440us void sytd2_compute_w_kernel\u0026lt;double, int=8, int=1\u0026gt;(double const *, int, double const *, double const *, int, double*) 1.83% 2.56571s 1384952 1.8520us 992ns 6.0160us [CUDA memcpy DtoH] 1.28% 1.79144s 305932 5.8550us 5.4070us 8.4800us void svd_row_rotate_batch_32x16\u0026lt;double\u0026gt;(int, int const *, int const *, int, double*, int, double const *, int*) 1.05% 1.47607s 1157165 1.2750us 831ns 8.8960us ptxcall_anonymous21_4 1.03% 1.44738s 62378 23.203us 10.560us 57.983us void geqr2_smem\u0026lt;double, double, int=8, int=6, int=4\u0026gt;(int, int, double*, unsigned long, double*, int) 0.72% 1.00992s 252101 4.0060us 1.5680us 805.62us void cutensor_internal_namespace::tensor_elementwise_kernel\u0026lt;cutensor_internal_namespace::ElementwiseConfig\u0026lt;unsigned int=1, int=128, unsigned int=64, unsigned int=2\u0026gt;, cutensor_internal_namespace::ElementwiseStaticOpPack\u0026lt;cutensorOperator_t=1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t\u0026gt;, double, double, double, double\u0026gt;(cutensor_internal_namespace::ElementwiseParameters, int, int, cutensorOperator_t=1, unsigned int=64 const *, cutensor_internal_namespace::ElementwiseParameters, unsigned int=2 const *, cutensor_internal_namespace::ElementwiseParameters, cutensor_internal_namespace::ElementwiseConfig\u0026lt;unsigned int=1, int=128, unsigned int=64, unsigned int=2\u0026gt; const *, unsigned int=2 const **, bool, bool, bool, bool, cutensor_internal_namespace::ElementwiseOpPack) 0.71% 990.60ms 46039 21.516us 9.3440us 52.063us void geqr2_smem\u0026lt;double, double, int=8, int=5, int=5\u0026gt;(int, int, double*, unsigned long, double*, int) 0.54% 748.94ms 139738 5.3590us 895ns 11.776us void syevj_parallel_order_set_kernel\u0026lt;int=512\u0026gt;(int, int*) 0.51% 707.93ms 35644 19.861us 12.192us 34.368us void geqr2_smem\u0026lt;double, double, int=8, int=7, int=3\u0026gt;(int, int, double*, unsigned long, double*, int) 0.49% 684.11ms 542074 1.2620us 927ns 5.9200us copy_info_kernel(int, int*) 0.48% 669.92ms 298670 2.2420us 1.2470us 10.144us [CUDA memcpy HtoD] This run was allowed to go on for longer to show that now the runtime on the GPU is dominated by useful work contracting tensors or computing factorizations.\nHead to head performance fight With a real physics use case to test, the nice folks over at NVIDIA ran a performance comparison for us. These are some representative (actually, rather small) simulations using our code, and you can see that the GPU acceleration is helping a lot.\n  CPU vs GPU PEPS Scaling   And now I'm able to run simulations that used to take a week in an hour thanks to the GPU acceleration, with no loss of accuracy so far. We've had a big success using the GPU, and so far haven't run up against the device memory limit. Having a Julia code makes it easier to maintain and easier to reason about new code as it's written. We're looking forward to using this backend to accelerate other tensor network algorithms and make it quicker to test out new ideas.\nSome Takeaways  The CuArrays.jl package makes it pretty easy and pleasant to integrate the GPU into your codebase. However, there are still some pain points (in the sparse linear algebra code especially) if someone is looking for a project to contribute to. It's important to pick a problem like this that is about 95% of the way to the perfect problem for the GPU if you want to brag without having to do much work. You should actually check to make sure that you aren't copying memory back and forth when you don't need to. If you are copying more than you think you should, you can try to figure out where it's coming from by inserting a stacktrace call into the cudaMemcpy calls at the CUDAdrv.jl package. That should tell you the location up the call stack in your code where the copy to/from the device is triggered.  ","date":1577731519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577731519,"objectID":"40c1c61bd7be3773ccf5adf9a2481c6c","permalink":"https://kshyatt.github.io/post/itensorsgpu/","publishdate":"2019-12-30T13:45:19-05:00","relpermalink":"/post/itensorsgpu/","section":"post","summary":"Last year at JuliaCon, Matt Fishman and I gave a talk about our ongoing effort to port the ITensor code from C++ to Julia. At the time, I mentioned that we had begun trying to integrate a GPU-based contraction backend and were looking forward to some significant speedups.\nWhat is ITensor To understand why we thought this would be fruitful, and some performance traps we could already anticipate before writing a single line of GPU code, you have to understand a bit about the problem we're trying to solve.","tags":[],"title":"Itensorsgpu","type":"post"},{"authors":["Katharine Hyatt"],"categories":null,"content":"","date":1575435393,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"b19b944608b13cdf35ef6b4464e1daf6","permalink":"https://kshyatt.github.io/talk/tnsaa/","publishdate":"2019-10-18T17:56:33-04:00","relpermalink":"/talk/tnsaa/","section":"talk","summary":"","tags":[],"title":"TNSAA 2019-2020","type":"talk"},{"authors":["Katharine Hyatt","E. Miles Stoudenmire"],"categories":null,"content":"","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"25a0ef7814e2d8be9bf0aeb4571e1e59","permalink":"https://kshyatt.github.io/publication/peps/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/publication/peps/","section":"publication","summary":"Tensor network algorithms have been remarkably successful solving a variety of problems in quantum many-body physics. However, algorithms to optimize two-dimensional tensor networks known as PEPS lack many of the aspects that make the seminal density matrix renormalization group (DMRG) algorithm so powerful for optimizing one-dimensional tensor networks known as matrix product states. We implement a framework for optimizing two-dimensional PEPS tensor networks which includes all of steps that make DMRG so successful for optimizing one-dimension tensor networks. We present results for several 2D spin models and discuss possible extensions and applications.","tags":[],"title":"DMRG Approach to Optimizing Two-Dimensional Tensor Networks","type":"publication"},{"authors":["Katharine Hyatt","Matthew Fishman"],"categories":null,"content":"","date":1563897608,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"0ac45e7dd78c406d947f039d0548660c","permalink":"https://kshyatt.github.io/talk/juliacon2019/","publishdate":"2019-10-21T12:00:08-04:00","relpermalink":"/talk/juliacon2019/","section":"talk","summary":"We present ITensors.jl, a ground-up rewrite of the C++ ITensor package for tensor network simulations in Julia. We will motivate the use of tensor networks in physics and give some examples for how ITensors.jl can help make the use and development of tensor network algorithms easier for researchers, users, and developers.","tags":[],"title":"Intelligent Tensors in Julia","type":"talk"},{"authors":[],"categories":null,"content":"","date":1563573830,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"43cefa7112b107cc5dc2d5449f89b4b2","permalink":"https://kshyatt.github.io/talk/uiuc2019/","publishdate":"2019-10-18T18:03:50-04:00","relpermalink":"/talk/uiuc2019/","section":"talk","summary":"","tags":[],"title":"DMRG Approach to Optimizing Two-Dimensional Tensor Networks","type":"talk"},{"authors":["Katharine Hyatt"],"categories":[],"content":"Getting into a new programming language is always a bit overwhelming, because there are so many places to start! Here is a short list of things I like to send to people new to Julia, aimed at varying skill levels:\n Learn X in Y Minutes - Julia edition, a good quick tour if you already know other languages like python pretty well Towards Julia 1.0, by David Sanders, is a great tutorial for the latest versions of Julia. Because the language went through so many changes before 1.0, not all older tutorials will still be usable! Getting Started with 1.0's Package Manager, for those you (probably everyone) who will want to start using libraries in the \u0026ldquo;package ecosystem\u0026rdquo; and maybe write your own! How to use the new iteration interface, since many people need to write for-loops! Celeste.jl \u0026ndash; amazing example of what mixed (shared and distributed memory) parallelism is capable of in Julia StaticArrays.jl, high performance small arrays which show off some of the features of julia's type parameterization JuliaGPU, which has a collection of packages for writing native Julia code for GPUs and using existing GPU libraries from Julia The JuliaLang learning page, which has lots of good links for learning the language and some best practices  ","date":1549035908,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"cf1b05e33765921bd09b6ce33da0cd06","permalink":"https://kshyatt.github.io/post/startingjulia/","publishdate":"2019-02-01T10:45:08-05:00","relpermalink":"/post/startingjulia/","section":"post","summary":"Getting into a new programming language is always a bit overwhelming, because there are so many places to start! Here is a short list of things I like to send to people new to Julia, aimed at varying skill levels:\n Learn X in Y Minutes - Julia edition, a good quick tour if you already know other languages like python pretty well Towards Julia 1.0, by David Sanders, is a great tutorial for the latest versions of Julia.","tags":[],"title":"Getting Started With Julia","type":"post"},{"authors":["Katharine Hyatt"],"categories":[],"content":"In this post I'm going to go through my step-by-step process of finding some code in base Julia which is not covered by tests, adding tests which do cover it, checking to make sure the tests pass, and finally opening a pull request to have my changes merged in to Julia itself. We'll be working off commit 0b0a394b3e7741d38f00dbb29895b6ba6a7d0767 if you want to follow along. (To do so, you can git checkout 0b0a394b3e7741d38f00dbb29895b6ba6a7d0767. Keep in mind this will put you in the dreaded detached HEAD state.) I do almost all my development on macOS and Linux, so some of the shell commands will be a little different if you're on Windows.\nPrereqs To get started, we're going to need a few things:\n Some basic git knowledge: git add, git clone, git commit, git checkout, and git push. If you need an introduction to git, some decent options are:  The GitHub help pages The official git docs The codecademy intro to git tutorial, which is free   A source build of Julia, from git clone git@github.com:JuliaLang/julia.git (I use SSH, HTTPS is fine too) A fork of Julia, since most people making their first pull requests will not have the ability to push branches to the main julia repository.  Zeroth Step: Setting Up Our Fork Having forked the main julia repository to your GitHub account, you're ready to create a local clone and let it know about both the upstream repos.\nI usually clone julia into a subdirectory of my home directory called projects, so that on a new machine what happens is:\ncd ~ # move to the home directory mkdir -p projects # create the projects directory if it doesn\u0026#39;t already exist, otherwise do nothing cd projects # move to the kshyatt/projects directory git clone git@github.com:JuliaLang/julia.git This will copy the repository from GitHub, including the current state of all the source code as well as all its history. We can look at the remotes of our clone to see other copies of the julia repository our clone is aware of (can fetch/pull/push against). Since we cloned from GitHub, the repository already knows about it:\n~/projects $ cd julia # move down into the julia directory post clone ~/projects/julia $ git remote -v # list remotes and their URLs origin\tgit@github.com:JuliaLang/julia.git (fetch) origin\tgit@github.com:JuliaLang/julia.git (push) If you were using HTTPS, this would look like:\n~/projects $ cd julia # move down into the julia directory post clone ~/projects/julia $ git remote -v # list remotes and their URLs origin\thttps://github.com/JuliaLang/julia.git (fetch) origin\thttps://github.com/JuliaLang/julia.git (push) So our repository knows about its \u0026ldquo;parent\u0026rdquo;. But we probably don't have the ability to write directly to the main Julia repositiory (yet \u0026ndash; who knows what the future holds?), which means we won't be able to create our own branches there. Instead, we will need to use a repository copy we do have the ability to write to \u0026ndash; our fork \u0026ndash; to make our coming changes visible to the wide world. If we cloned from the JuliaLang repo, we need to add another remote for our fork. Usually this will have a URL like: https://github.com/$USERNAME/julia.git or git@github.com:$USERNAME/julia.git where $USERNAME is your GitHub username. To let git know about the second remote, we use git remote add:\ngit remote add $REMOTE_NAME git@github.com:$USERNAME/julia.git $REMOTE_NAME is the local alias we want to call the second remote by. The remote we cloned from (JuliaLang) gets the alias origin. You can pick any alias you like which doesn't conflict with an existing one. I often use kshyatt because even in the grim depths of 3:24am I can usually remember my own name. So when I do the remote add, it looks like:\ngit remote add kshyatt git@github.com:kshyatt/julia.git And afterwards, if we look at the remotes git knows about:\n~/projects/julia $ git remote -v # list remotes and their URLs kshyatt\tgit@github.com:kshyatt/julia.git (fetch) kshyatt\tgit@github.com:kshyatt/julia.git (push) origin\tgit@github.com:JuliaLang/julia.git (fetch) origin\tgit@github.com:JuliaLang/julia.git (push) And again, with HTTPS this would be:\n~/projects/julia $ git remote -v # list remotes and their URLs kshyatt\thttps://github.com/kshyatt/julia.git (fetch) kshyatt\thttps://github.com/kshyatt/julia.git (push) origin\thttps://github.com/JuliaLang/julia.git (fetch) origin\thttps://github.com/JuliaLang/julia.git (push) With a fork and local knowledge of it, we're ready to find some changes to make.\nThe Hunt for Red Lines There are many kinds of contributions a person can make to an open source project:\n New features Performance improvements Bug fixes Test coverage Documentation improvements Usage examples  If you're just starting out contributing to Julia, and don't have an obvious idea for a feature or a performance enhancement you could write, writing tests or improving docs are a great way to get started. People often disagree about the desirability of a new feature but I almost never see anyone say we should have fewer tests! Writing tests is a good way to learn how Julia works \u0026ndash; or doesn't, if you find a bug. We don't want to write tests just for the sake of having them, though \u0026ndash; we want to test as many julia features as possible in as little time as possible. If we're writing new tests, it's always a good idea to keep the test coverage in mind. Test coverage measures which lines of base (and stdlib) Julia are exercised by the tests. We keep this information current at codecov. Here's a screenshot:\nWe can see the attractive sunburst picture, and a history of recent commits showing their total coverage percentage. On a lark, I picked base/ to go hunting for uncovered lines.\nHere we see each file, with its total lines, covered lines, uncovered lines, and coverage percentage. If we want to increase coverage, we want to pick a file with less than 100% of its lines covered. Keep in mind that these numbers may be less than the true coverage, because right now the coverage-enabled runs of the tests are all done on Unix and some code in Julia is Windows or BSD specific. Tests for such code aren't run by the coverage measurement script.\nI happened to scroll down until I came across base/secretbuffer.jl. This file had a lot of uncovered lines, so I thought I had a decent chance of finding some code I could write tests to cover:\nClicking on the Codecov link opens up an annotated version of the source file, with green coloring representing lines which are covered by tests, and red coloring representing lines which are not. Scrolling down the file, I came across the write method, which writes some data packed into a UInt8 into the secret buffer's data:\nIn order to test this, it would probably be good to have some sense of what it does. Having been using Julia since 2015, I probably have a bad heuristic for how easy this is to understand \u0026ndash; if you are looking at it and can't figure it out in about 5 minutes, there's absolutely no shame in logging onto the Julia Slack and asking! We have #my-first-pr and #helpdesk channels exactly for this. The type itself is defined earlier in the file:\nmutable struct SecretBuffer \u0026lt;: IO data::Vector{UInt8} size::Int ptr::Int function SecretBuffer(; sizehint=128) s = new(Vector{UInt8}(undef, sizehint), 0, 1) finalizer(final_shred!, s) return s end end My attempt at an explanation\n SecretBuffer, like the name implies, is a type which contains some data which you'd like to keep secret from prying eyes It contains information about the total amount of information stored, in the size field It contains information about where in the data we are currently reading or writing - the pointer field  This analogy is a little fuzzy, but if you're confused, imagine the buffer is a piece of paper on which we've written a message in code. The size is the total number of letters in the message, and the pointer is the letter we are trying to encode or decode at the moment. Then the write function attempts to insert an unsigned 8 bit integer b at the ptr of the input SecretBuffer. The uncovered lines are there to deal with the case where the SecretBuffer's data array is totally full (the ptr is past its length). In the paper analogy, this would mean that we wrote all the way to the bottom right corner and need to turn over to a fresh page.\nWriting the test To test this case, what I chose to do was:\n Create a small SecretBuffer and write up to its data size Write a little more data, so that it will have to call this untested code and extend data in-place Make sure all the data I wrote survived the process  Julia's tests live in the test/ directory, which is in the top-level with base/ and stdlib/. From my julia clone, I can see what it contains:\nls ~/projects/julia/test If you do this, you'll see there is a file called test/secretbuffer.jl. That looks promising, and indeed, it's where tests for SecretBuffer are (as opposed to tests for another kind of buffer, put there as a prank). Now you can open your favorite text editor and modify the file. I like to see if there is an obvious place to add the tests in the file \u0026ndash; for example, if I'm testing a function for subtracting complex numbers, and there are already a bunch of tests for adding them, that would be a natural location to add my test.\nJulia test files tend to be organized into testsets, which group similar tests and make it easier to pinpoint where the problem is when one fails. Since we're testing something specific, it makes sense to add our own little testset nested inside the main SecretBuffers testset (as the others are):\n@testset \u0026#34;write!pastdatasize\u0026#34; begin # ready to test some stuff in here! end Testsets get a name, so that if they fail we can see what's broken. It's best to write something descriptive for this rather than \u0026ldquo;aaaaa\u0026rdquo;, \u0026ldquo;aaaab\u0026rdquo;, etc. Following what I wrote above, I need to create a small SecretBuffer\u0026hellip;\n@testset \u0026#34;write!pastdatasize\u0026#34; begin sb = SecretBuffer(sizehint=2) end Where sizehint tells Julia how big to make the initial internal data array (you can see this in the type definition I copied above). Now I need to write some data to the buffer to read the end of the data array, which has two slots currently. I picked the biggest integer UInt8 can represent, because its bits will all be non-zero (here I am anticipating what I'm going to test at the end).\n@testset \u0026#34;write!pastdatasize\u0026#34; begin sb = SecretBuffer(sizehint=2) # data vector will not grow bits = typemax(UInt8) write(sb, bits) write(sb, bits) end Now the data vector is fully saturated, and we will test the growing part of it by trying to write one more time:\n@testset \u0026#34;write!pastdatasize\u0026#34; begin sb = SecretBuffer(sizehint=2) # data vector will not grow bits = typemax(UInt8) write(sb, bits) write(sb, bits) # data vector must grow write(sb, bits) end Finally, it would be good to make sure this write didn't silently fail somehow or corrupt the previously written data, so we are going to read back the buffer and make sure its contents are what we expect, which is a String with three identical elements:\n@testset \u0026#34;write!pastdatasizeandunsafe_convert\u0026#34; begin sb = SecretBuffer(sizehint=2) # data vector will not grow bits = typemax(UInt8) write(sb, bits) write(sb, bits) # data vector must grow write(sb, bits) seek(sb, 0) @test read(sb, String) == \u0026#34;\\xff\\xff\\xff\u0026#34; shred!(sb) end We seek back to the start of the data to make sure we capture all three elements. We read the buffer as a String and compare with what the result should be. How did I know it would be three \\xff?\njulia\u0026gt; String([typemax(UInt8)]) \u0026#34;\\xff\u0026#34; We can make sure the test works by running it. I nearly always use the Makefile to do this, because it will catch whitespace errors, which I make prodiguously.\ncd ~/projects/julia make test-secretbuffer You can run any test in test/ this way, for example make test-arrayops works too. Assuming the tests pass, we are ready to commit and move on. If they don't pass, you'll need to look at the error message and try to figure out how to fix the test and/or the code it's supposed to be testing.\nCommiting and pushing We need to commit our changes and push them to a remote so that we can open a pull request, which will let someone review the proposed changes and (hopefully) merge them. Although it's possible to do this from your master branch, this can very quickly lead to tears. It's better to:\n Make a new git branch Commit the changes there Push that branch to your fork Open a pull request from the branch on your fork  We can use git checkout -b to both create a new branch and move over onto it, which will drag all our changes along with. git checkout -b is a shorter way of saying git branch $BRANCH_NAME \u0026amp;\u0026amp; git checkout $BRANCH_NAME. You can choose whatever you like for the branch name, but I tend to prepend with my initials and then use a short description of what the changes were. As an example:\ngit checkout -b ksh/sbtest Now we're ready to commit. Since the change was just to one file, and pretty small, we can do a command-line commit and bypass the editor. We'll commit and pass the -m flag with an accompanying commit message, which will tell everyone what the changes do.\ngit commit test/secretbuffer.jl -m \u0026#34;Added test for write-ing past the pre-existing data length\u0026#34; I told git which file(s) to use in the commit and the message to attach to them. The commit will be made to the ksh/sbtest branch. Now I'm ready to push. Because, for the purposes of this example, I don't have write access to JuliaLang/julia, I will push to my fork at kshyatt/julia and open the pull request from there. All I have to do is tell git to push to the kshyatt remote I created at the start of this post.\n~/projects/julia $ git push kshyatt ksh/sbtest Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 613 bytes | 306.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for \u0026#39;ksh/sbtest\u0026#39; on GitHub by visiting: remote: https://github.com/kshyatt/julia/pull/new/ksh/sbtest remote: To github.com:kshyatt/julia.git * [new branch] ksh/sbtest -\u0026gt; ksh/sbtest Hooray! The push to my remote succeeded. The changes have been propagated \u0026ldquo;upstream\u0026rdquo; and can be used to open the pull request.\nImportant Note: Make sure to open the pull request from JuliaLang/julia, and not from your fork!\nTo open the PR, I open up the main Julia repo and lo and behold, GitHub has a suggestion for me! I sure do want to make this pull request, so I hit the green button that says \u0026ldquo;Compare and Pull Request\u0026rdquo;.\nThat leads to the comparison screen, where I can see my proposed changes and write some comments describing what they do.\nGitHub has also helpfully used my commit message to title the pull request, and I think it's quite a nice title so I will leave it be. This PR is pretty self-explanatory so I won't write a description in the big text box provided, but it doesn't hurt to do this if you think it might not be obvious what your code does. Don't worry about requesting a reviewer or adding labels if this is your first PR - a maintainer can handle that for you. If everything looks ok, press the \u0026ldquo;Create pull request\u0026rdquo; button. At this point, you can wait for some reviews and maintainers can give you detailed help if your PR needs some changes. It's very common for a PR to need some tweaks \u0026ndash; it happens to me all the time! Although it can feel a little discouraging to have to keep making changes, maintainers are devoting the time to re-reviewing your PR because they think it's good and want it to be the best it can be (just like Julia as a whole). If you want to see the real-life PR that I created in making this post, it's right here.\nLess than half a minute after I opened the pull request, super speedy reviewer Matt Bauman thought it was alright to merge it.\nNow we just had to wait for the continuous integration (CI) servers to run the tests. There are CI servers for Windows, Linux, macOS, ARM, and freeBSD. If \u0026ldquo;the lights turn green\u0026rdquo; (all tests passed), and someone has approved the PR, it should be merged very soon. If 24 hours have passed and it hasn't been merged, feel free to ping the person who approved the PR and ask them if they can merge it for you. Sometimes these things fall by the wayside. Even if CI fails, it may be an incidental failure unrelated to your changes. If it is related, someone will be happy to help you figure out what the problem is, or you can dig through the logs yourself to try to figure it out. If no one reviews your PR after a day or two, you can post a comment on the PR asking for review, or come onto Slack and bug us about it.\n","date":1548959966,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"3cc9b4da6386823a768b3fa8fcedf796","permalink":"https://kshyatt.github.io/post/firstjuliapr/","publishdate":"2019-01-31T13:39:26-05:00","relpermalink":"/post/firstjuliapr/","section":"post","summary":"In this post I'm going to go through my step-by-step process of finding some code in base Julia which is not covered by tests, adding tests which do cover it, checking to make sure the tests pass, and finally opening a pull request to have my changes merged in to Julia itself. We'll be working off commit 0b0a394b3e7741d38f00dbb29895b6ba6a7d0767 if you want to follow along. (To do so, you can git checkout 0b0a394b3e7741d38f00dbb29895b6ba6a7d0767.","tags":[],"title":"Making a first Julia pull request","type":"post"},{"authors":["Katharine Hyatt"],"categories":null,"content":"","date":1548959150,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"018391579050435f7beb4b61b4922fcb","permalink":"https://kshyatt.github.io/talk/juliacon2017/","publishdate":"2019-01-31T13:25:50-05:00","relpermalink":"/talk/juliacon2017/","section":"talk","summary":"Start using Julia to do simulations of quantum systems with many interacting particles! We will write a single-core exact diagonalization code which can handle a variety of models from quantum physics, using Julia to make it readable and performant. We'll tour the Julia package ecosystem for useful packages that will help us store our results to share with others and get to the interesting physics. Then, we'll use some of Julia's parallel features to scale up our code to do many-body simulation on many-core and many-node systems in a convenient, reproducible, and fast way. You need not ever have written any Julia before. We'll use physics examples as natural motivation to explore Julia's capabilities, but no background in quantum mechanics is required. We will introduce the models as we go.","tags":[],"title":"From One To Many","type":"talk"},{"authors":["Katharine Hyatt"],"categories":[],"content":"A Journey Through Laziness Quite often, I'm working at my desktop in my campus office. The compute cluster on campus does not allow me to SSH directly to nodes or to spawn jobs, so usually you SSH into the head node and queue the jobs (which may be julia jobs, if I want to run some fancy parallel Julia code).\nThis is kind of a pain, so I tried out using ClusterManagers.jl and julia's native addprocs and remote worker functionality to automate this a bit.\nI have key-based login set up from desktop to cluster (the head node), and from cluster to its worker nodes. Let's call my script my_bad_science.jl (I read somewhere that just like in the code itself, your writing about the code should contain descriptive names).\nFirst, I wanted to open a REPL on the desktop (I could make this a script too, of course). All the below examples are julia 0.5.0. Then I opened an SSH tunnel to the cluster:\njulia\u0026gt; addprocs([(\u0026#34;cluster_hostname\u0026#34;,1)], tunnel=true, max_parallel=1, exename=\u0026#34;/home/kshyatt/julia/v0.5/julia\u0026#34;, sshflags=\u0026#34;-vv\u0026#34;) I passed -vv (verbose) to SSH because otherwise the Julia worker tends to time out. exename is there to tell julia that it's not installed in the same place on the cluster as it is on my desktop.\nThe output looks like:\n# a bunch of verbose SSH chat happens here # removed to protect the guilty/innocent 1-element Array{Int64,1}: 2 Just like any other call to addprocs. Perfect! Now, on the remote head node, which is my worker, I will pull in my script:\n# my bad science script using ClusterManagers.jl # our cluster uses PBS so ClusterManagers.addprocs_pbs(12) # now that the workers will have been loaded # I need to import the packages I need to do # the cool stuff using Distributions using research_utils # a private package of mine #fill up some parameter arrays Ls = collect(8:2:20) w = 10. # disorder strength d = Uniform(-w, w) disorder = [rand(d, L) for L in Ls] Hs = pmap(makeHamiltonians, Ls, disorder) # some code down here to write out the # Hamiltonians to HDF5 - it\u0026#39;s gross :( What this will do is spawn a 12 worker job on the cluster, do the work, and write the results on the cluster. I could also have this script return something to the master worker on my desktop, if I wanted (perhaps timing information?).\nFinally, on my desktop REPL, all I need to do is:\njulia\u0026gt; rr = @spawnat 2 include(\u0026#34;my_bad_science.jl\u0026#34;) Future(2,1,5,Nullable{Any}()) julia\u0026gt; wait(rr) @spawnat returns a Future, and we need to wait for it to be finished. Or, we could schedule it and merrily go on our way, periodically checking to see if rr is finished.\nOne could also imagine a fancier version of this, plotting data as it comes in from the cluster head node on one's desktop (so we could use something a bit prettier than UnicodePlots.jl). For that you might need a RemoteChannel.\nThis is pretty short but it dramatically improved my lazing about, watching-job-results-come-in workflow. I hope it's useful to someone else!\n","date":1548952064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"f9b632c06110e44c8a5c028e888706c3","permalink":"https://kshyatt.github.io/post/sshcluster/","publishdate":"2019-01-31T11:27:44-05:00","relpermalink":"/post/sshcluster/","section":"post","summary":"A Journey Through Laziness Quite often, I'm working at my desktop in my campus office. The compute cluster on campus does not allow me to SSH directly to nodes or to spawn jobs, so usually you SSH into the head node and queue the jobs (which may be julia jobs, if I want to run some fancy parallel Julia code).\nThis is kind of a pain, so I tried out using ClusterManagers.","tags":[],"title":"Running Cluster Jobs Remotely ","type":"post"},{"authors":["Katharine Hyatt"],"categories":null,"content":"","date":1548951748,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"4a4bf0b426ed7c8c7bfd98dc3a0af1ae","permalink":"https://kshyatt.github.io/talk/juliacon2015/","publishdate":"2019-01-31T11:22:28-05:00","relpermalink":"/talk/juliacon2015/","section":"talk","summary":"Using high-performance computing to probe quantum systems is becoming more and more common in condensed matter physics research. Many of the commonly used languages and techniques in this space are either difficult to learn or not performant. Julia has allowed us to quickly develop and test codes for a variety of commonly used algorithms, including exact diagonalization, DMRG, and quantum Monte Carlo. Its parallel features, including MPI and GPGPU integration, make it particularly attractive for many quantum simulations. I’ll discuss what features of Julia have been most useful for us when working on these simulations and the developments we’re most excited about.","tags":[],"title":"Quantum Statistical Simulations with Julia","type":"talk"},{"authors":["Katharine Hyatt"],"categories":null,"content":"","date":1520979259,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"eb76aebcfdd7662cdafaf9c0e68b117a","permalink":"https://kshyatt.github.io/talk/boston2018/","publishdate":"2019-10-18T18:14:19-04:00","relpermalink":"/talk/boston2018/","section":"talk","summary":"","tags":[],"title":"Writing an entanglement geometry dictionary: tensor networks, geodesy, and quenching","type":"talk"},{"authors":["Katharine Hyatt","James R. Garrison","Bela Bauer"],"categories":null,"content":"","date":1485878472,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"546e5f96678ea0415d892e70fc83a5e1","permalink":"https://kshyatt.github.io/publication/disentangling/","publishdate":"2017-01-31T11:01:12-05:00","relpermalink":"/publication/disentangling/","section":"publication","summary":"Tensor networks impose a notion of geometry on the entanglement of a quantum system. In some cases, this geometry is found to reproduce key properties of holographic dualities, and subsequently much work has focused on using tensor networks as tractable models for holographic dualities. Conventionally, the structure of the network - and hence the geometry - is largely fixed a priori by the choice of tensor network ansatz. Here, we evade this restriction and describe an unbiased approach that allows us to extract the appropriate geometry from a given quantum state. We develop an algorithm that iteratively finds a unitary circuit that transforms a given quantum state into an unentangled product state. We then analyze the structure of the resulting unitary circuits. In the case of non-interacting, critical systems in one dimension, we recover signatures of scale invariance in the unitary network, and we show that appropriately defined geodesic paths between physical degrees of freedom exhibit known properties of a hyperbolic geometry.","tags":[],"title":"Extracting Entanglement Geometry from Quantum States","type":"publication"},{"authors":["Katharine Hyatt","James R. Garrison","Andrew C. Potter","Bela Bauer"],"categories":null,"content":"","date":1454256072,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"31b1e1476eda403563e74d032f53dab1","permalink":"https://kshyatt.github.io/publication/bilayer/","publishdate":"2016-01-31T11:01:12-05:00","relpermalink":"/publication/bilayer/","section":"publication","summary":"In the presence of strong disorder and weak interactions, closed quantum systems can enter a many-body localized phase where the system does not conduct, does not equilibrate even for arbitrarily long times, and robustly violates quantum statistical mechanics. The starting point for such a many-body localized phase is usually taken to be an Anderson insulator where, in the limit of vanishing interactions, all degrees of freedom of the system are localized. Here, we instead consider a model where in the noninteracting limit, some degrees of freedom are localized while others remain delocalized. Such a system can be viewed as a model for a many-body localized system brought into contact with a small bath of a comparable number of degrees of freedom. We numerically and analytically study the effect of interactions on this system and find that generically, the entire system delocalizes. However, we find certain parameter regimes where results are consistent with localization of the entire system, an effect recently termed many-body proximity effect.","tags":[],"title":"Many-body localization in the presence of a small bath","type":"publication"},{"authors":["Ann B. Kallin","Katharine Hyatt","Rajiv R. P. Singh","Roger G. Melko"],"categories":null,"content":"","date":1328025677,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571674833,"objectID":"c1e355421cfbe1301f4012ad2e6b9217","permalink":"https://kshyatt.github.io/publication/nlce/","publishdate":"2012-01-31T11:01:17-05:00","relpermalink":"/publication/nlce/","section":"publication","summary":"We develop a method to calculate the bipartite entanglement entropy of quantum models, in the thermodynamic limit, using a Numerical Linked Cluster Expansion (NLCE) involving only rectangular clusters. It is based on exact diagonalization of all n x m rectangular clusters at the interface between entangled subsystems A and B. We use it to obtain the Renyi entanglement entropy of the two-dimensional transverse field Ising model, for arbitrary real Renyi index alpha. Extrapolating these results as a function of the order of the calculation, we obtain universal pieces of the entanglement entropy associated with lines and corners at the quantum critical point. They show NLCE to be one of the few methods capable of accurately calculating universal properties of arbitrary Renyi entropies at higher dimensional critical points.","tags":[],"title":"Entanglement at a Two-Dimensional Quantum Critical Point: a Numerical Linked Cluster Expansion Study","type":"publication"}]