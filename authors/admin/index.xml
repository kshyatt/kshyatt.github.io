<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Katharine Hyatt</title>
    <link>https://kshyatt.github.io/authors/admin/</link>
      <atom:link href="https://kshyatt.github.io/authors/admin/index.xml" rel="self" type="application/rss+xml" />
    <description>Katharine Hyatt</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Mon, 02 Mar 2020 00:00:31 -0400</lastBuildDate>
    <image>
      <url>https://kshyatt.github.io/img/icon-192.png</url>
      <title>Katharine Hyatt</title>
      <link>https://kshyatt.github.io/authors/admin/</link>
    </image>
    
    <item>
      <title>March Meeting 2020</title>
      <link>https://kshyatt.github.io/talk/aps2020/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:31 -0400</pubDate>
      <guid>https://kshyatt.github.io/talk/aps2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accelerating Tensor Computations in Julia with the GPU</title>
      <link>https://kshyatt.github.io/post/itensorsgpu/</link>
      <pubDate>Mon, 30 Dec 2019 13:45:19 -0500</pubDate>
      <guid>https://kshyatt.github.io/post/itensorsgpu/</guid>
      <description>&lt;h1 id=&#34;introductionroadmap&#34;&gt;Introduction/Roadmap&lt;/h1&gt;
&lt;p&gt;Last year at JuliaCon, Matt Fishman and I gave a &lt;a href=&#34;https://www.youtube.com/watch?v=A2ypJkA26co&#34;&gt;talk&lt;/a&gt; about our ongoing effort to port the &lt;a href=&#34;http://itensor.org&#34;&gt;ITensor&lt;/a&gt; code from C++ to Julia. At the time, I mentioned that we had begun trying to integrate a GPU-based tensor contraction backend and were looking forward to some significant speedups. We ended up completing this integration, and saw runtimes for representative parameters go from one week to one hour. In this post I&#39;m going to go over:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The physics problem we were writing code to solve&lt;/li&gt;
&lt;li&gt;Why the GPU is a good candidate to accelerate our simulations to solve this problem&lt;/li&gt;
&lt;li&gt;Why Julia is a natural choice if you want to take advantage of the GPU easily&lt;/li&gt;
&lt;li&gt;How, after we wrote the initial implementation, we made it faster by removing roadblocks&lt;/li&gt;
&lt;li&gt;Some final thoughts about why this worked out well for us&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-is-itensor&#34;&gt;What is ITensor&lt;/h1&gt;
&lt;p&gt;To understand why we thought this would be fruitful, and some performance traps we could already anticipate before writing a single line of GPU code, you have to understand a bit about the problem we&#39;re trying to solve.
Tensor network algorithms are a very active area of research at the intersection of condensed matter physics, high energy physics, quantum information, and computer science. DMRG, which is the most successful numerical method for condensed matter systems in 1D, wasn&#39;t originally formulated as a tensor network algorithm, but &amp;ldquo;tensor network speak&amp;rdquo; turns out to be a natural language with which to discuss DMRG and its descendants. (If you&#39;re not sure what it&#39;s meant to solve, imagine a long chain of quantum objects, each with a fretful relationship with their neighbours. Will they overcome their differences and work together to form a magnet, or continue arguing with each other and fail to come to a consensus? DMRG is an efficient way to answer this question.) All this is to say that there&#39;s this class of algorithms physicists (and increasingly computer scientists) are interested in and they work well.&lt;/p&gt;
&lt;p&gt;The driving idea of a tensor network algorithm is to take some high-dimensional optimization problem (solving for groundstates (eigenvectors corresponding to the minimal eigenvalue) of quantum many body systems is an example of this, since the full dimension of the system grows exponentially in the number of constituent particles) and compress it down to a much lower dimensional problem while retaining most of the important features. We do this by taking a &lt;code&gt;d^N&lt;/code&gt; length vector, where &lt;code&gt;d&lt;/code&gt; is the number of degrees of freedom of each constituent, and &lt;code&gt;N&lt;/code&gt; is the number of constituents, and breaking it down into a set of multidimensionsal tensors, the number of which hopefully scales like &lt;code&gt;N&lt;/code&gt; or at least much less than exponentially.&lt;/p&gt;
&lt;p&gt;If you&#39;ve studied linear algebra before, you&#39;ve seen some simple examples of tensors: scalars, vectors, and matrices. We say a scalar is a 0-rank tensor, a vector a 1-rank tensor (since it has one index), a matrix a 2-rank tensor (since it has two indices), and then there are higher rank tensors, with three or more indices. When we multiply two matrices &lt;code&gt;A_ij&lt;/code&gt; and &lt;code&gt;B_jk&lt;/code&gt; together to get &lt;code&gt;C_ik&lt;/code&gt;, we&#39;re performing a &amp;ldquo;tensor contratction&amp;rdquo;, and we compute &lt;code&gt;C_ik&lt;/code&gt; by ssumming over indices &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;j&lt;/code&gt;. Similarly, if we had high-rank tensors &lt;code&gt;A_ijkl&lt;/code&gt; and &lt;code&gt;B_lmin&lt;/code&gt;, we could contract them to get &lt;code&gt;C_jkmn&lt;/code&gt; - again, by summing over indices &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;l&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Most tensor network algorithms are based on performing this decomposition and then iteratively improving it towards a target vector. Usually in physics that target is a physical state, but tensor networks have also been used for machine learning tasks and can represent quantum error correcting codes as well.&lt;/p&gt;
&lt;p&gt;ITensor is a C++ package dedicated to providing both high level algorithms using tensor networks, like DMRG, and the low-level building blocks to create your own. ITensor makes it easy to create tensors out of indices, perform linear algebra factorizations (such as QR or SVD) on them, without forcing the user to worry about index ordering. You can read the ITensor tutorials for more information or watch our talk.&lt;/p&gt;
&lt;h1 id=&#34;why-use-the-gpu&#34;&gt;Why use the GPU&lt;/h1&gt;
&lt;p&gt;It seems pretty reasonable that you could expect a speedup for many tensor network algorithms by using a GPU. By permuting indices, it&#39;s possible to reduce all contraction operations to matrix-matrix or matrix-vector multiplications, at which the GPU excels. Most tensor network algorithms have runtimes dominated by such contractions or by SVD. However, there are some performance gotchas we always need to consider when using the GPU:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The device has comparatiely low memory. The most expensive cards have 32GB of onboard RAM, which is a lot, but many state-of-the-art DMRG calculations require over a terabyte of RAM or checkpointing by writing intermediate information to disk.&lt;/li&gt;
&lt;li&gt;There&#39;s high latency and low bandwidth for memory transfers. If we absolutely &lt;em&gt;have to&lt;/em&gt; copy memory from the host CPU to the device, we should try to do it all in one big blob, and not in many small chunks. Although the GPU can overlap computations and memory transfers writing code to handle this can be a bit complex.&lt;/li&gt;
&lt;li&gt;The performance for single precision floats is much better than for double precision. Although we&#39;ll probably see a perfomance boost for doubles, it won&#39;t be as dramatic as for single precision &lt;code&gt;Float32&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, there is a danger in the most naive approach to handling tensor contractions, which is to just permute all involved tensors into the index layout necessary to write the operation as a matrix-matrix multiplication, and then sit back and call &lt;code&gt;GEMM&lt;/code&gt;. Although in many cases this will work quite well, especially if the permutation doesn&#39;t involve many indices, there are plenty of bad cases where a great deal of time could be spent alllocating destination arrays and permuting source arrays into them. The risk of this increases with the average number of indices on each tensor (since there are more &amp;ldquo;bad&amp;rdquo; permutations available).&lt;/p&gt;
&lt;p&gt;For these reasons, we weren&#39;t sure if the GPU would be a good choice for my current reseach project. You can read the physics details &lt;a href=&#34;https://arxiv.org/abs/1908.08833&#34;&gt;here&lt;/a&gt;. We have a C++ implementation of this code which is CPU only and runs about 5000 lines. One of my goals with the project was to eventually open source the code in the hope that others might find it useful or improve it. However, C++, despite being a great langauge, can be intimidating to many people. I have some experience writing C code that uses CUDA which, despite the powerful API and really granular control over the device the programmer is provided, can also be intimidating and require you to keep a lot of balls in the air while you&#39;re writing the code. But the C++ solution, stuck on a single node as it was, with all parallelism coming from CPU BLAS spread over 28 cores, was taking up to a week to run to get a decent picture of the converged result. This was pretty frustrating from a development perspective because it meant the debug cycle of &amp;ldquo;something&#39;s wrong&amp;rdquo; - &amp;ldquo;OK, think I found it&amp;rdquo; - &amp;ldquo;is this a fix?&amp;rdquo; - &amp;ldquo;nope, something&#39;s still wrong&amp;rdquo; had to take place over multiple days.&lt;/p&gt;
&lt;p&gt;Since Miles (the original author of ITensor) and Matt were already thinking of rewriting ITensor in Julia (see our talk for the motivations for this decision), I decided I would try to help and maybe try to add some GPU support to the new package. Many tensor network algorithms, not only this one, are dominated by tensor-tensor contractions as mentioned above. And since I had already had some experience working with Julia&#39;s GPU programming/wrapping infrastructre in &lt;a href=&#34;https://github.com/JuliaGPU/CuArrays.jl&#34;&gt;&lt;code&gt;CuArrays.jl&lt;/code&gt;&lt;/a&gt;, I thought it wouldn&#39;t be so hard to integrate a GPU based tensor operations backend to &lt;a href=&#34;https://github.com/ITensor/ITensors.jl&#34;&gt;&lt;code&gt;ITensors.jl&lt;/code&gt;&lt;/a&gt;. (In fact, we sometimes want to add or subtract tensors, not just contract them.)&lt;/p&gt;
&lt;p&gt;Our first approach, and one I don&#39;t have benchmarks for, was the naive method described above - just permute everything and call &lt;code&gt;CUBLAS&lt;/code&gt;&#39;s general matrix-matrix multiplication routine. In general, handling GPU memory with &lt;code&gt;CuArrays.jl&lt;/code&gt; was very easy. An &lt;code&gt;ITensor&lt;/code&gt; is essentially an opaque &lt;code&gt;Vector&lt;/code&gt; with some indices along for the ride, which tell you in what order to index elements of the &lt;code&gt;Vector&lt;/code&gt;. It&#39;s analogous to &lt;code&gt;CartesianArray&lt;/code&gt; for those who have used Julia&#39;s multidimensional array support. Since our algorithms usually require us to somehow achieve a contraction, QR decomposition, and addition, we thought treating the &lt;code&gt;ITensor&lt;/code&gt; storage as essentially a blob you can permute and give to multiplication API calls would be enough. Usually in these algorithms you&#39;re not often accessing or manipulating single elements or slices of the &lt;code&gt;ITensor&lt;/code&gt; (although this is possible to do and easy in both the C++ and Julia versions), just the tensors themselves.&lt;/p&gt;
&lt;p&gt;Here&#39;s the sum total of the code I needed to get a barebones &lt;code&gt;cuITensor&lt;/code&gt; that you could move on and off the GPU:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Type&lt;/span&gt;{T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Dense&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{float&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)}(CuArrays&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(float&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;),dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Type&lt;/span&gt;{T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;} &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;))

cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(is&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)   &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;,is&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;))

cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;()
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{N&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;, N&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}
    dat&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CuVector&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{float&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)}(undef&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
    fill!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dat&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, float&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
    ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Dense&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}(dat&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;} &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;))

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Array&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Dense&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Dense&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Array&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},   inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;} &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;))
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {S&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;} &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;))
cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; Base&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;collect&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
    typeof&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(collect&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Mostly, this handles different ways of providing the indices, and a few options for the input data type. I assumed that if you called the &lt;code&gt;cuITensor&lt;/code&gt; constructor but gave it an input CPU array, you probably wanted that array transferred to the GPU.
That&#39;s the easy part. Adding support for some other operations, like QR decomposition or eigensolving, wasn&#39;t much harder:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; eigenHermitian&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{ElT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;};
                        kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {ElT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}
  maxdim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;maxdim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,minimum&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dims&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)))
  mindim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;mindim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
  cutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;cutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;)
  absoluteCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Bool&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;absoluteCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;)
  doRelCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Bool&lt;/span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(kwargs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;doRelCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;local&lt;/span&gt; DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, UM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ElT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Complex&lt;/span&gt;
    DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, UM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUSOLVER&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;heevd!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;V&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;U&amp;#39;&lt;/span&gt;, matrix&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
    DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, UM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUSOLVER&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;syevd!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;V&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;U&amp;#39;&lt;/span&gt;, matrix&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  DM_&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reverse&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  truncerr&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, docut&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; truncate!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(DM_&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;;maxdim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;maxdim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, cutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, absoluteCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;absoluteCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, doRelCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;doRelCutoff&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  dD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; length&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reverse&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(UM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, dims&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; dD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; size&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
      dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CuMatrix&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;dD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;])
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# Make the new indices to go onto U and V&lt;/span&gt;
  u&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;     &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; eltype&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)(dD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  v&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;     &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; eltype&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)(dD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  Uinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;((ind&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;),u&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  Dinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; IndsT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;((u&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,v&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  dV_&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;   &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CuArrays&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(ElT&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  copyto!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV_&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, vec&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  U&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Dense&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(dV_&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;),Uinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  D&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Diag&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(real&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;(DM&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)),Dinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; U&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,D&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This weird looking method of getting &lt;code&gt;dV&lt;/code&gt; into &lt;code&gt;dV_&lt;/code&gt; is necessary because of the way &lt;code&gt;CuArrays.jl&lt;/code&gt; deals with reshapes. As we&#39;ll see later it doesn&#39;t seem to impact performance too much.
But of course, the big problem we wanted to solve was contractions. Because the CPU code also works by performing the permutation and calling &lt;code&gt;GEMM&lt;/code&gt;, it was relatively easy to pirate that over to the GPU:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; contract!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(C&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},
                   p&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CProps&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,
                   A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},
                   B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;},
                   α&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Tα&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;,
                   β&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Tβ&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,Tα&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,Tβ&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;}

  &lt;span style=&#34;color:#75715e&#34;&gt;# bunch of code to find permutations and permute α and β goes here!&lt;/span&gt;
  CUBLAS&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;gemm_wrapper!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(cref&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, tA&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,tB&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,aref&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,bref&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,promote_type&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,Tα&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)(α&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;),promote_type&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,Tβ&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)(β&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))

  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permuteC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
    permutedims!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(C&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,reshape&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(cref&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,p&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;newCrange&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;),p&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The design of &lt;a href=&#34;https://github.com/ITensor/ITensors.jl&#34;&gt;&lt;code&gt;ITensors.jl&lt;/code&gt;&lt;/a&gt; specifies that the &lt;code&gt;ITensor&lt;/code&gt; type itself is not specialized on its storage type, so that from the user&#39;s point of view, they have a tensor-like object contracting with another tensor-like object, and the developers can worry about how to multiply a diagonal-like rank-6 tensor by a sparse rank-4 tensor. This makes it easier for users to implement the algorithms they need to do their research in, and it&#39;s one of the library&#39;s strengths. All that was needed to allow an &lt;code&gt;ITensor&lt;/code&gt; with GPU-backed storage to play nicely with an &lt;code&gt;ITensor&lt;/code&gt; with CPU-backed storage was few lines of edge case handling:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; contract!!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(R&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, T1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;DenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsT1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, T2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsT2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; contract!!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(R&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(cu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), labelsT1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, T2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, labelsT2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) 
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; contract!!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(R&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, T1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsT1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, T2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;DenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Number&lt;/span&gt;,N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}, labelsT2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;NTuple&lt;/span&gt;{N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}) where&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; {NR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,N1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,N2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; contract!!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(R&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, T1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, labelsT1&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(cu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(T2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), labelsT2&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) 
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I chose to copy the CPU storage to the device before the addition or contraction, hoping that this would occur rarely and that the performance gain in the main operation would offset the memory transfer time. Ideally this situation should never occur: we absolutely want to minimize memory transfers. However, if a user makes a mistake and forgets a &lt;code&gt;cuITensor(A)&lt;/code&gt;, their code won&#39;t error out. In fact, in the latest version of &lt;a href=&#34;https://github.com/ITensor/ITensorsGPU.jl&#34;&gt;&lt;code&gt;ITensorsGPU.jl&lt;/code&gt;&lt;/a&gt; this dubious feature is disallowed, since in my code it was always the result of forgetting to initialize something on the GPU which should have been.&lt;/p&gt;
&lt;p&gt;This was enough to get the barebones GPU support working. But I was still worried about the issue with the permutations, especially because the week-long simulations are those which are most memory intensive, and I was worried about running out of space on the device. Could there be a better solution?&lt;/p&gt;
&lt;h1 id=&#34;cutensor-and-the-story-of-how-computers-made-my-labour-useless&#34;&gt;&lt;code&gt;CUTENSOR&lt;/code&gt; and the story of how computers made my labour useless&lt;/h1&gt;
&lt;p&gt;Around this time we became aware of &lt;a href=&#34;https://docs.nvidia.com/cuda/cutensor/index.html&#34;&gt;CUTENSOR&lt;/a&gt;, an NVIDIA library designed exactly for our use case: adding and contracting high rank tensors with indices in arbitrary order. However, this library was, of course, written in C. Luckily Julia makes it pretty easy to wrap C APIs, and we got started doing so in &lt;a href=&#34;https://github.com/JuliaGPU/CuArrays.jl/pull/330&#34;&gt;this epic PR&lt;/a&gt; to &lt;code&gt;CuArrays.jl&lt;/code&gt;. &lt;code&gt;CuArrays.jl&lt;/code&gt; already provides nice high- and low-level wrappers of CUDA C libraries in Julia, not only for dense or sparse linear algebra but also for random number generation and neural network primitives. So adding a multi-dimensional array library was a natural fit. During the process of getting these wrappers into a state fit for a public facing library, Tim Besard created some very nice scripts which automate the process of creating Julia wrappers for C functions, &lt;a href=&#34;https://github.com/JuliaGPU/CuArrays.jl/pull/421&#34;&gt;automating away&lt;/a&gt; many hours of labour I had performed years ago to get the sparse linear algebra and solver libraries working. Sic transit gloria mundi, I guess (generating these wrappers was not a glorious process). Now it will be easy for us to integrate changes to the &lt;code&gt;CUTENSOR&lt;/code&gt; API over time as more features are added.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;CUTENSOR&lt;/code&gt;&#39;s internals handle matching up elements for products and sums as part of the contraction process, so the permutations that &lt;code&gt;ITensors.jl&lt;/code&gt; performs for a CPU-based &lt;code&gt;ITensor&lt;/code&gt; are unnecessary. By overriding a few functions we&#39;re able to call the correct internal routines which feed through to &lt;code&gt;CUTENSOR&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; Base&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;(B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;CuDenseTensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  opC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUTENSOR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CUTENSOR_OP_IDENTITY&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
  opA&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUTENSOR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CUTENSOR_OP_IDENTITY&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
  opAC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CUTENSOR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CUTENSOR_OP_ADD&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
  Ais&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  Bis&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  ind_dict&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Vector&lt;/span&gt;{Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;}()
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (idx&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, i&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; enumerate&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(inds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
      push!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(ind_dict&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, i&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  Adata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  Bdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  reshapeBdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reshape&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,dims&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bis&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  reshapeAdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; reshape&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Adata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,dims&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Ais&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  &lt;span style=&#34;color:#75715e&#34;&gt;# probably a silly way to handle this, but it worked&lt;/span&gt;
  ctainds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Ais&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  ctbinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;, length&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bis&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (ii&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; enumerate&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Ais&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
      ctainds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[ii&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; findfirst&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ind_dict&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (ii&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ib&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; enumerate&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bis&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
      ctbinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[ii&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; findfirst&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ib&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ind_dict&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
  ctcinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; copy&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(ctbinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  C&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CuArrays&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(eltype&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), dims&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bis&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  CUTENSOR&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;elementwiseBinary!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(one&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(eltype&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Adata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), reshapeAdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ctainds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, opA&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, one&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(eltype&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(Bdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), reshapeBdata&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ctbinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, opC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, C&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, ctcinds&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, opAC&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
  copyto!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), vec&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(C&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once these wrappers and their tests were merged into &lt;code&gt;CuArrays.jl&lt;/code&gt;, I set about changing up how we were calling the contraction functions on the &lt;code&gt;ITensors.jl&lt;/code&gt; side. We decided to do this because within &lt;code&gt;CUTENSOR&lt;/code&gt; there were already highly optimized routines for various permutations, and we didn&#39;t want to try to reinvent the wheel with our permute-then-GEMM system. Switching to &lt;code&gt;CUTENSOR&lt;/code&gt; let us abstract away the permutation-handling, so the code interfacing with &lt;code&gt;CuArrays.jl&lt;/code&gt; was much simpler than under our previous approach. Dealing with optional dependencies, as &lt;code&gt;CuArrays.jl&lt;/code&gt; would have been for &lt;code&gt;ITensors.jl&lt;/code&gt;, is still kind of a pain in Julia, so I made a new package, &lt;a href=&#34;https://github.com/ITensor/ITensorsGPU.jl&#34;&gt;&lt;code&gt;ITensorsGPU.jl&lt;/code&gt;&lt;/a&gt;, to hold all the CUDA-related logic. What&#39;s nice is that from the end-user&#39;s perspective, they just have copy the tensors to the GPU at the start of the simulation and afterwards everything works mostly seamlessly &amp;ndash; they don&#39;t have to concern themselves with index orders or anything. It frees the user to focus more on high-level algorithm design.&lt;/p&gt;
&lt;h1 id=&#34;extirpating-memory-copies&#34;&gt;Extirpating memory copies&lt;/h1&gt;
&lt;p&gt;Copying memory back and forth from the device is extremely slow, and the code will perform best if we can eliminate as many as possible. One way to see how much time the device is spending on them is using NVIDIA&#39;s &lt;a href=&#34;https://docs.nvidia.com/cuda/profiler-users-guide/index.html&#34;&gt;&lt;code&gt;nvprof&lt;/code&gt;&lt;/a&gt; tool. Working with the cluster means I usually do most of my development over SSH, so I used command line mode, which is really easy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;nvprof ~/software/julia/julia prof_run.jl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This generates some output about how much time the GPU spent doing various things, which is very long horizontally - scroll the box sideways if you can&#39;t see the function names:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1386746&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Profiling application: julia prof_run.jl
&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1386746&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Profiling result:
            Type  Time&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;%&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;      Time     Calls       Avg       Min       Max  Name
 GPU activities:   14.02%  12.1800s     &lt;span style=&#34;color:#ae81ff&#34;&gt;80792&lt;/span&gt;  150.76us  102.69us  408.00us  void sytd2_upper_cta&amp;lt;double, double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;5&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, double*, int, double*, double*, double*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    7.44%  6.46842s   &lt;span style=&#34;color:#ae81ff&#34;&gt;5077903&lt;/span&gt;  1.2730us  1.1190us  54.367us  &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;CUDA memcpy HtoD&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
                    6.02%  5.22885s   &lt;span style=&#34;color:#ae81ff&#34;&gt;4430677&lt;/span&gt;  1.1800us     959ns  7.2640us  ptxcall_anonymous19_1
                    5.06%  4.39301s    &lt;span style=&#34;color:#ae81ff&#34;&gt;178968&lt;/span&gt;  24.546us  8.4480us  81.855us  void cutensor_internal_namespace::tensor_contraction_kernel&amp;lt;cutensor_internal_namespace::tc_config_t&amp;lt;int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;32, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;32, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0&amp;gt;, double, double, double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; const *, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; const *, cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    4.97%  4.31812s    &lt;span style=&#34;color:#ae81ff&#34;&gt;346589&lt;/span&gt;  12.458us  5.4400us  29.088us  void cutensor_internal_namespace::tensor_contraction_kernel&amp;lt;cutensor_internal_namespace::tc_config_t&amp;lt;int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0&amp;gt;, double, double, double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; const *, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; const *, cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    4.96%  4.31109s   &lt;span style=&#34;color:#ae81ff&#34;&gt;2397744&lt;/span&gt;  1.7970us  1.7270us  6.8160us  ptxcall_setindex_kernel__26
                    4.18%  3.63043s    &lt;span style=&#34;color:#ae81ff&#34;&gt;228988&lt;/span&gt;  15.854us  5.8560us  28.672us  void cutensor_internal_namespace::tensor_contraction_kernel&amp;lt;cutensor_internal_namespace::tc_config_t&amp;lt;int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0&amp;gt;, double, double, double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; const *, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; const *, cutensor_internal_namespace::tc_params_t, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    3.41%  2.96655s   &lt;span style=&#34;color:#ae81ff&#34;&gt;2162240&lt;/span&gt;  1.3710us  1.2470us  3.4880us  &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;CUDA memcpy DtoH&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
                    2.77%  2.40280s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1827148&lt;/span&gt;  1.3150us  1.0870us  7.3600us  ptxcall_anonymous19_14
                    2.70%  2.34219s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1327280&lt;/span&gt;  1.7640us  1.5360us  6.5920us  ptxcall_setindex_kernel__15
                                                                                                                                                                                                                       5424,21       96%
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can see the &lt;code&gt;memcpyHtoD&lt;/code&gt; there, and it&#39;s taking up a lot of time! By carefully going through and creating &lt;code&gt;ITensors&lt;/code&gt; with the appropriate storage type in internal routines, like so:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# again probably a nicer way to do this&lt;/span&gt;
is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; all&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;([data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(store&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[i&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;,j&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;])) &lt;span style=&#34;color:#66d9ef&#34;&gt;isa&lt;/span&gt; CuArray&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Ny&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, j&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Nx&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
N&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; spinI&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(findindex&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(A&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[row&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, col&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;], &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;S&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;); is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; spinI&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;Index&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;; is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Bool&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
    I_data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; CuArrays&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; zeros&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
    idi&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;         &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; diagind&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(reshape&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(I_data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)), &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
    I_data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;[idi&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; CuArrays&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; ones&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Float64&lt;/span&gt;, dim&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;))
    I           &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; is_gpu&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; cuITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;( I_data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#39;&lt;/span&gt;) ) &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; ITensor&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(vec&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(I_data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), IndexSet&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#39;&lt;/span&gt;))
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; I
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;it&#39;s possible to dramatically cut down on this, getting a final profiling report of&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3303697&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Profiling application: julia prof_run.jl
&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3303697&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Profiling result:
            Type  Time&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;%&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;      Time     Calls       Avg       Min       Max  Name
 GPU activities:   13.78%  19.2803s    &lt;span style=&#34;color:#ae81ff&#34;&gt;343307&lt;/span&gt;  56.160us  15.328us  7.2638ms  cutensor_internal_namespace::contraction_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::KernelParam_double_iden_1_2_false_false_double_iden_1_2_false_false_double_1_double_double_tb_128_128_8_simt_sm50_256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                   13.47%  18.8464s    &lt;span style=&#34;color:#ae81ff&#34;&gt;440326&lt;/span&gt;  42.800us  15.071us  653.75us  cutensor_internal_namespace::contraction_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::KernelParam_double_iden_1_2_false_false_double_iden_1_2_true_false_double_1_double_double_tb_128_128_8_simt_sm50_256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    9.70%  13.5765s    &lt;span style=&#34;color:#ae81ff&#34;&gt;262876&lt;/span&gt;  51.646us  15.360us  197.02us  cutensor_internal_namespace::contraction_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_true_false_double_1_double_double_tb_128_128_8_simt_sm50_256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    8.90%  12.4562s    &lt;span style=&#34;color:#ae81ff&#34;&gt;114666&lt;/span&gt;  108.63us  15.648us  4.7480ms  cutensor_internal_namespace::contraction_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_false_false_double_1_double_double_tb_128_128_8_simt_sm50_256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    7.96%  11.1327s    &lt;span style=&#34;color:#ae81ff&#34;&gt;305932&lt;/span&gt;  36.389us  22.559us  76.831us  void gesvdbj_batch_32x16&amp;lt;double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int const *, int const *, int const *, int, double const *, int, double, double*, double*, int*, double, int, double&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    5.15%  7.21024s   &lt;span style=&#34;color:#ae81ff&#34;&gt;2439080&lt;/span&gt;  2.9560us  2.0480us  6.8470us  void ormtr_gerc&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;5, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, double const *, int, int, double*, unsigned long, double const *, int, double const *&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    3.62%  5.06010s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1663200&lt;/span&gt;  3.0420us  1.6630us  5.8240us  void sytd2_symv_upper&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, double const *, double const *, unsigned long, double const *, double*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    3.43%  4.80411s   &lt;span style=&#34;color:#ae81ff&#34;&gt;2439080&lt;/span&gt;  1.9690us  1.5680us  5.4390us  void ormtr_gemv_c&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int, double const *, unsigned long, double const *, int, double*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.84%  3.97216s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1636800&lt;/span&gt;  2.4260us  2.1120us  5.6000us  void larfg_kernel_fast&amp;lt;double, double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;6&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, double*, double*, int, double*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.57%  3.59984s   &lt;span style=&#34;color:#ae81ff&#34;&gt;2123225&lt;/span&gt;  1.6950us     864ns  90.111us  &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;CUDA memcpy DtoD&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
                    2.37%  3.31885s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1663200&lt;/span&gt;  1.9950us  1.1520us  4.5760us  void sytd2_her2k_kernel&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, double*, unsigned long, double const *, int, double const *&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.25%  3.14692s    &lt;span style=&#34;color:#ae81ff&#34;&gt;611864&lt;/span&gt;  5.1430us  4.6390us  12.704us  void svd_column_rotate_batch_32x16&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;5, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int const *, int const *, int, double*, int, double*, int, double const *, int*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.17%  3.03782s     &lt;span style=&#34;color:#ae81ff&#34;&gt;54611&lt;/span&gt;  55.626us  3.1990us  315.74us  void cutensor_internal_namespace::reduction_kernel&amp;lt;bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;6, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;256, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;256, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, double, double, double, double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;double, double const *, double const *, cutensor_internal_namespace::reduction_kernel&amp;lt;bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;6, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;256, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;256, bool&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, double, double, double, double, double&amp;gt;, double const *, double const **, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensor_internal_namespace::reduction_params_t&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.11%  2.95838s     &lt;span style=&#34;color:#ae81ff&#34;&gt;29720&lt;/span&gt;  99.541us  30.367us  1.3328ms  cutensor_internal_namespace::contraction_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::KernelParam_double_iden_1_2_true_false_double_iden_1_2_false_true_double_1_double_double_tb_128_128_8_simt_sm50_256&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    2.08%  2.91405s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1663200&lt;/span&gt;  1.7520us  1.5040us  4.5440us  void sytd2_compute_w_kernel&amp;lt;double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;double const *, int, double const *, double const *, int, double*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    1.83%  2.56571s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1384952&lt;/span&gt;  1.8520us     992ns  6.0160us  &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;CUDA memcpy DtoH&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
                    1.28%  1.79144s    &lt;span style=&#34;color:#ae81ff&#34;&gt;305932&lt;/span&gt;  5.8550us  5.4070us  8.4800us  void svd_row_rotate_batch_32x16&amp;lt;double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int const *, int const *, int, double*, int, double const *, int*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    1.05%  1.47607s   &lt;span style=&#34;color:#ae81ff&#34;&gt;1157165&lt;/span&gt;  1.2750us     831ns  8.8960us  ptxcall_anonymous21_4
                    1.03%  1.44738s     &lt;span style=&#34;color:#ae81ff&#34;&gt;62378&lt;/span&gt;  23.203us  10.560us  57.983us  void geqr2_smem&amp;lt;double, double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;6, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;4&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int, double*, unsigned long, double*, int&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.72%  1.00992s    &lt;span style=&#34;color:#ae81ff&#34;&gt;252101&lt;/span&gt;  4.0060us  1.5680us  805.62us  void cutensor_internal_namespace::tensor_elementwise_kernel&amp;lt;cutensor_internal_namespace::ElementwiseConfig&amp;lt;unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;128, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2&amp;gt;, cutensor_internal_namespace::ElementwiseStaticOpPack&amp;lt;cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t, cutensorOperator_t&amp;gt;, double, double, double, double&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cutensor_internal_namespace::ElementwiseParameters, int, int, cutensorOperator_t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; const *, cutensor_internal_namespace::ElementwiseParameters, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; const *, cutensor_internal_namespace::ElementwiseParameters, cutensor_internal_namespace::ElementwiseConfig&amp;lt;unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;128, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;64, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2&amp;gt; const *, unsigned int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; const **, bool, bool, bool, bool, cutensor_internal_namespace::ElementwiseOpPack&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.71%  990.60ms     &lt;span style=&#34;color:#ae81ff&#34;&gt;46039&lt;/span&gt;  21.516us  9.3440us  52.063us  void geqr2_smem&amp;lt;double, double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;5, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;5&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int, double*, unsigned long, double*, int&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.54%  748.94ms    &lt;span style=&#34;color:#ae81ff&#34;&gt;139738&lt;/span&gt;  5.3590us     895ns  11.776us  void syevj_parallel_order_set_kernel&amp;lt;int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;512&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.51%  707.93ms     &lt;span style=&#34;color:#ae81ff&#34;&gt;35644&lt;/span&gt;  19.861us  12.192us  34.368us  void geqr2_smem&amp;lt;double, double, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;8, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;7, int&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;3&amp;gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int, double*, unsigned long, double*, int&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.49%  684.11ms    &lt;span style=&#34;color:#ae81ff&#34;&gt;542074&lt;/span&gt;  1.2620us     927ns  5.9200us  copy_info_kernel&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;int, int*&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
                    0.48%  669.92ms    &lt;span style=&#34;color:#ae81ff&#34;&gt;298670&lt;/span&gt;  2.2420us  1.2470us  10.144us  &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;CUDA memcpy HtoD&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This run was allowed to go on for longer to show that now the runtime on the GPU is dominated by useful work contracting tensors or computing factorizations.&lt;/p&gt;
&lt;h1 id=&#34;head-to-head-performance-fight&#34;&gt;Head to head performance fight&lt;/h1&gt;
&lt;p&gt;With a real physics use case to test, the nice folks over at NVIDIA ran a performance comparison for us. These are some representative (actually, rather small) simulations using our code, and you can see that the GPU acceleration is helping a lot.&lt;/p&gt;













&lt;figure&gt;

&lt;img src=&#34;scaling.png&#34; alt=&#34;&#34; &gt;


  
  
  &lt;figcaption&gt;
    CPU vs GPU PEPS Scaling
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The &lt;code&gt;x&lt;/code&gt;-axis in this figure is a little opaque - it refers to varying problem sizes we might want to simulate, specifically an &lt;code&gt;N&lt;/code&gt; by &lt;code&gt;N&lt;/code&gt; lattice with a maximum tensor interior dimension of &lt;code&gt;m&lt;/code&gt; - thus, &lt;code&gt;N - m&lt;/code&gt; in the labels.
And now I&#39;m able to run simulations that used to take a week in an hour thanks to the GPU acceleration, with no loss of accuracy so far. We&#39;ve had a big success using the GPU, and so far haven&#39;t run up against the device memory limit. Having a Julia code makes it easier to maintain and easier to reason about new code as it&#39;s written. We&#39;re looking forward to using this backend to accelerate other tensor network algorithms and make it quicker to test out new ideas.&lt;/p&gt;
&lt;h1 id=&#34;some-takeaways&#34;&gt;Some Takeaways&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;CuArrays.jl&lt;/code&gt; package makes it pretty easy and pleasant to integrate the GPU into your codebase. However, there are still some pain points (in the sparse linear algebra code especially) if someone is looking for a project to contribute to.&lt;/li&gt;
&lt;li&gt;It&#39;s important to pick a problem like this that is about 95% of the way to the perfect problem for the GPU if you want to brag without having to do much work.&lt;/li&gt;
&lt;li&gt;You should actually check to make sure that you aren&#39;t copying memory back and forth when you don&#39;t need to. If you are copying more than you think you should, you can try to figure out where it&#39;s coming from by inserting a &lt;code&gt;stacktrace&lt;/code&gt; call into the &lt;code&gt;cudaMemcpy&lt;/code&gt; calls at the &lt;a href=&#34;https://github.com/JuliaGPU/CUDAdrv.jl/blob/75bf4e4385bd2c431080ec501b9ae6f3d6c771ec/src/memory.jl&#34;&gt;&lt;code&gt;CUDAdrv.jl&lt;/code&gt;&lt;/a&gt; package. That should tell you the location up the call stack in your code where the copy to/from the device is triggered.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There were several things that made this transition successful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The simplicity of writing Julia wrappers for C code (this let us quickly interface with &lt;code&gt;CUTENSOR&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;The existing ease of use of &lt;code&gt;CuArrays.jl&lt;/code&gt; - now it&#39;s easy to extend this to a new library, and it made writing the constructors for our own types quite straightforward&lt;/li&gt;
&lt;li&gt;The fact that our problem was well-suited for the GPU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&#39;s certainly true that we could have achieved the same, or possibly better, had we modified the C++ ITensor code to use the GPU. But I think it&#39;s fair to say it would have taken much more time, and would have been less accessible to other people in condensed matter physics. We were willing to settle for a slightly less than optimal speedup if the code to achieve it got written at all.&lt;/p&gt;
&lt;p&gt;If you want to look at some of the unpleasant internals of this, feel free to check out &lt;a href=&#34;https://github.com/ITensor/ITensorsGPU.jl&#34;&gt;&lt;code&gt;ITensorsGPU.jl&lt;/code&gt;&lt;/a&gt; and try things out for yourself. If you&#39;re interested in learning more about tensor network algorithms, check out Miles&amp;rsquo; site &lt;a href=&#34;http://tensornetwork.org&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TNSAA 2019-2020</title>
      <link>https://kshyatt.github.io/talk/tnsaa/</link>
      <pubDate>Wed, 04 Dec 2019 00:56:33 -0400</pubDate>
      <guid>https://kshyatt.github.io/talk/tnsaa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DMRG Approach to Optimizing Two-Dimensional Tensor Networks</title>
      <link>https://kshyatt.github.io/publication/peps/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://kshyatt.github.io/publication/peps/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Intelligent Tensors in Julia</title>
      <link>https://kshyatt.github.io/talk/juliacon2019/</link>
      <pubDate>Tue, 23 Jul 2019 12:00:08 -0400</pubDate>
      <guid>https://kshyatt.github.io/talk/juliacon2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Getting Started With Julia</title>
      <link>https://kshyatt.github.io/post/startingjulia/</link>
      <pubDate>Fri, 01 Feb 2019 10:45:08 -0500</pubDate>
      <guid>https://kshyatt.github.io/post/startingjulia/</guid>
      <description>&lt;p&gt;Getting into a new programming language is always a bit overwhelming, because there are so many places to start! Here is a short list of things I like to send to people new to Julia, aimed at varying skill levels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learnxinyminutes.com/docs/julia/&#34;&gt;Learn X in Y Minutes - Julia edition&lt;/a&gt;, a good quick tour if you already know other languages like python pretty well&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=puMIBYthsjQ&#34;&gt;Towards Julia 1.0&lt;/a&gt;, by &lt;a href=&#34;https://github.com/dpsanders&#34;&gt;David Sanders&lt;/a&gt;, is a great tutorial for the latest versions of Julia. Because the language went through so many changes before 1.0, not all older tutorials will still be usable!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/blog/2018/09/Pkgtutorial&#34;&gt;Getting Started with 1.0&#39;s Package Manager&lt;/a&gt;, for those you (probably everyone) who will want to start using libraries in the &amp;ldquo;package ecosystem&amp;rdquo; and maybe write your own!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/blog/2018/07/iterators-in-julia-0.7&#34;&gt;How to use the new iteration interface&lt;/a&gt;, since many people need to write &lt;code&gt;for&lt;/code&gt;-loops!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jeff-regier/Celeste.jl&#34;&gt;Celeste.jl&lt;/a&gt; &amp;ndash; amazing example of what mixed (shared and distributed memory) parallelism is capable of in Julia&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/JuliaArrays/StaticArrays.jl&#34;&gt;StaticArrays.jl&lt;/a&gt;, high performance small arrays which show off some of the features of julia&#39;s type parameterization&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/juliagpu&#34;&gt;JuliaGPU&lt;/a&gt;, which has a collection of packages for writing native Julia code for GPUs and using existing GPU libraries from Julia&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://julialang.org/learning/&#34;&gt;The JuliaLang learning page&lt;/a&gt;, which has lots of good links for learning the language and some best practices&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Making a first Julia pull request</title>
      <link>https://kshyatt.github.io/post/firstjuliapr/</link>
      <pubDate>Thu, 31 Jan 2019 13:39:26 -0500</pubDate>
      <guid>https://kshyatt.github.io/post/firstjuliapr/</guid>
      <description>&lt;p&gt;In this post I&#39;m going to go through my step-by-step process of finding some code in base Julia which is not covered by tests, adding tests which do cover it, checking to make sure the tests pass, and finally opening a pull request to have my changes merged in to Julia itself. We&#39;ll be working off commit &lt;code&gt;0b0a394b3e7741d38f00dbb29895b6ba6a7d0767&lt;/code&gt; if you want to follow along. (To do so, you can &lt;code&gt;git checkout 0b0a394b3e7741d38f00dbb29895b6ba6a7d0767&lt;/code&gt;. Keep in mind this will put you in the dreaded detached &lt;code&gt;HEAD&lt;/code&gt; state.) I do almost all my development on macOS and Linux, so some of the shell commands will be a little different if you&#39;re on Windows.&lt;/p&gt;
&lt;h1 id=&#34;prereqs&#34;&gt;Prereqs&lt;/h1&gt;
&lt;p&gt;To get started, we&#39;re going to need a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some basic &lt;code&gt;git&lt;/code&gt; knowledge: &lt;code&gt;git add&lt;/code&gt;, &lt;code&gt;git clone&lt;/code&gt;, &lt;code&gt;git commit&lt;/code&gt;, &lt;code&gt;git checkout&lt;/code&gt;, and &lt;code&gt;git push&lt;/code&gt;. If you need an introduction to &lt;code&gt;git&lt;/code&gt;, some decent options are:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://help.github.com/articles/git-and-github-learning-resources/&#34;&gt;The GitHub help pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/docs&#34;&gt;The official git docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.codecademy.com/learn/learn-git&#34;&gt;The codecademy intro to git tutorial&lt;/a&gt;, which is free&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A source build of Julia, from &lt;code&gt;git clone git@github.com:JuliaLang/julia.git&lt;/code&gt; (I use SSH, HTTPS is fine too)&lt;/li&gt;
&lt;li&gt;A fork of Julia, since most people making their first pull requests will not have the ability to push branches to the main julia repository.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;zeroth-step-setting-up-our-fork&#34;&gt;Zeroth Step: Setting Up Our Fork&lt;/h1&gt;
&lt;p&gt;Having &lt;a href=&#34;https://help.github.com/articles/fork-a-repo/&#34;&gt;forked&lt;/a&gt; the &lt;a href=&#34;https://github.com/julialang/julia&#34;&gt;main julia repository&lt;/a&gt; to your GitHub account, you&#39;re ready to create a local &lt;a href=&#34;https://git-scm.com/docs/git-clone&#34;&gt;clone&lt;/a&gt; and let it know about both the upstream repos.&lt;/p&gt;
&lt;p&gt;I usually clone julia into a subdirectory of my home directory called &lt;code&gt;projects&lt;/code&gt;, so that on a new machine what happens is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd ~ &lt;span style=&#34;color:#75715e&#34;&gt;# move to the home directory&lt;/span&gt;
mkdir -p projects &lt;span style=&#34;color:#75715e&#34;&gt;# create the projects directory if it doesn&amp;#39;t already exist, otherwise do nothing&lt;/span&gt;
cd projects &lt;span style=&#34;color:#75715e&#34;&gt;# move to the kshyatt/projects directory&lt;/span&gt;
git clone git@github.com:JuliaLang/julia.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will copy the repository from GitHub, including the current state of all the source code as well as all its history. We can look at the &lt;em&gt;remotes&lt;/em&gt; of our clone to see other copies of the julia repository our clone is aware of (can fetch/pull/push against). Since we cloned from GitHub, the repository already knows about it:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/projects $ cd julia &lt;span style=&#34;color:#75715e&#34;&gt;# move down into the julia directory post clone&lt;/span&gt;
~/projects/julia $ git remote -v &lt;span style=&#34;color:#75715e&#34;&gt;# list remotes and their URLs&lt;/span&gt;
origin	git@github.com:JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	git@github.com:JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you were using HTTPS, this would look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/projects $ cd julia &lt;span style=&#34;color:#75715e&#34;&gt;# move down into the julia directory post clone&lt;/span&gt;
~/projects/julia $ git remote -v &lt;span style=&#34;color:#75715e&#34;&gt;# list remotes and their URLs&lt;/span&gt;
origin	https://github.com/JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	https://github.com/JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So our repository knows about its &amp;ldquo;parent&amp;rdquo;. But we probably don&#39;t have the ability to write directly to the main Julia repositiory (yet &amp;ndash; who knows what the future holds?), which means we won&#39;t be able to create our own branches there. Instead, we will need to use a repository copy we do have the ability to write to &amp;ndash; our fork &amp;ndash; to make our coming changes visible to the wide world. If we cloned from the JuliaLang repo, we need to add another remote for our fork. Usually this will have a URL like: &lt;code&gt;https://github.com/$USERNAME/julia.git&lt;/code&gt; or &lt;code&gt;git@github.com:$USERNAME/julia.git&lt;/code&gt; where &lt;code&gt;$USERNAME&lt;/code&gt; is your GitHub username. To let &lt;code&gt;git&lt;/code&gt; know about the second remote, we use &lt;a href=&#34;https://git-scm.com/docs/git-remote&#34;&gt;&lt;code&gt;git remote add&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git remote add $REMOTE_NAME git@github.com:$USERNAME/julia.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;$REMOTE_NAME&lt;/code&gt; is the &lt;em&gt;local&lt;/em&gt; alias we want to call the second remote by. The remote we cloned from (JuliaLang) gets the alias &lt;code&gt;origin&lt;/code&gt;. You can pick any alias you like which doesn&#39;t conflict with an existing one. I often use &lt;code&gt;kshyatt&lt;/code&gt; because even in the grim depths of 3:24am I can usually remember my own name. So when I do the remote add, it looks like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git remote add kshyatt git@github.com:kshyatt/julia.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And afterwards, if we look at the remotes &lt;code&gt;git&lt;/code&gt; knows about:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/projects/julia $ git remote -v &lt;span style=&#34;color:#75715e&#34;&gt;# list remotes and their URLs&lt;/span&gt;
kshyatt	git@github.com:kshyatt/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
kshyatt	git@github.com:kshyatt/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	git@github.com:JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	git@github.com:JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And again, with HTTPS this would be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/projects/julia $ git remote -v &lt;span style=&#34;color:#75715e&#34;&gt;# list remotes and their URLs&lt;/span&gt;
kshyatt	https://github.com/kshyatt/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
kshyatt	https://github.com/kshyatt/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	https://github.com/JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;fetch&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
origin	https://github.com/JuliaLang/julia.git &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;push&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With a fork and local knowledge of it, we&#39;re ready to find some changes to make.&lt;/p&gt;
&lt;h1 id=&#34;the-hunt-for-red-lines&#34;&gt;The Hunt for Red Lines&lt;/h1&gt;
&lt;p&gt;There are many kinds of contributions a person can make to an open source project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New features&lt;/li&gt;
&lt;li&gt;Performance improvements&lt;/li&gt;
&lt;li&gt;Bug fixes&lt;/li&gt;
&lt;li&gt;Test coverage&lt;/li&gt;
&lt;li&gt;Documentation improvements&lt;/li&gt;
&lt;li&gt;Usage examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&#39;re just starting out contributing to Julia, and don&#39;t have an obvious idea for a feature or a performance enhancement you could write, writing tests or improving docs are a great way to get started. People often disagree about the desirability of a new feature but I almost never see anyone say we should have fewer tests! Writing tests is a good way to learn how Julia works &amp;ndash; or doesn&#39;t, if you find a bug. We don&#39;t want to write tests just for the sake of having them, though &amp;ndash; we want to test as many julia features as possible in as little time as possible. If we&#39;re writing new tests, it&#39;s always a good idea to keep the test coverage in mind. Test coverage measures which lines of base (and stdlib) Julia are exercised by the tests. We keep this information current at &lt;a href=&#34;https://codecov.io/gh/julialang/julia&#34;&gt;codecov&lt;/a&gt;. Here&#39;s a screenshot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;codecovlanding.png&#34; alt=&#34;Codecov landing page&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can see the attractive sunburst picture, and a history of recent commits showing their total coverage percentage. On a lark, I picked &lt;code&gt;base/&lt;/code&gt; to go hunting for uncovered lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;basecov.png&#34; alt=&#34;Base coverage&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here we see each file, with its total lines, covered lines, uncovered lines, and coverage percentage. If we want to increase coverage, we want to pick a file with less than 100% of its lines covered. Keep in mind that these numbers may be less than the true coverage, because right now the coverage-enabled runs of the tests are all done on Unix and some code in Julia is Windows or BSD specific. Tests for such code aren&#39;t run by the coverage measurement script.&lt;/p&gt;
&lt;p&gt;I happened to scroll down until I came across &lt;code&gt;base/secretbuffer.jl&lt;/code&gt;. This file had a lot of uncovered lines, so I thought I had a decent chance of finding some code I could write tests to cover:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;findingsecretbuffer.png&#34; alt=&#34;Secretbuffer&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clicking on the Codecov link opens up an annotated version of the source file, with green coloring representing lines which are covered by tests, and red coloring representing lines which are not. Scrolling down the file, I came across the &lt;code&gt;write&lt;/code&gt; method, which writes some data packed into a &lt;code&gt;UInt8&lt;/code&gt; into the secret buffer&#39;s data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sbwritemissing.png&#34; alt=&#34;Red lines&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to test this, it would probably be good to have some sense of what it does. Having been using Julia since 2015, I probably have a bad heuristic for how easy this is to understand &amp;ndash; if you are looking at it and can&#39;t figure it out in about 5 minutes, there&#39;s absolutely no shame in logging onto the &lt;a href=&#34;https://julialang.slack.com/&#34;&gt;Julia Slack&lt;/a&gt; and asking! We have &lt;code&gt;#my-first-pr&lt;/code&gt; and &lt;code&gt;#helpdesk&lt;/code&gt; channels exactly for this. The type itself is defined earlier in the file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mutable&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;IO&lt;/span&gt;
    data&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Vector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;}
    size&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;
    ptr&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Int&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;function&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(; sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;)
        s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; new&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;Vector&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;}(undef&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;), &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        finalizer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(final_shred!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; s&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;My attempt at an explanation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SecretBuffer&lt;/code&gt;, like the name implies, is a type which contains some &lt;code&gt;data&lt;/code&gt; which you&#39;d like to keep secret from prying eyes&lt;/li&gt;
&lt;li&gt;It contains information about the total amount of information stored, in the &lt;code&gt;size&lt;/code&gt; field&lt;/li&gt;
&lt;li&gt;It contains information about where in the &lt;code&gt;data&lt;/code&gt; we are currently reading or writing - the &lt;code&gt;pointer&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This analogy is a little fuzzy, but if you&#39;re confused, imagine the buffer is a piece of paper on which we&#39;ve written a message in code. The &lt;code&gt;size&lt;/code&gt; is the total number of letters in the message, and the &lt;code&gt;pointer&lt;/code&gt; is the letter we are trying to encode or decode at the moment. Then the &lt;code&gt;write&lt;/code&gt; function attempts to insert an unsigned 8 bit integer &lt;code&gt;b&lt;/code&gt; at the &lt;code&gt;ptr&lt;/code&gt; of the input &lt;code&gt;SecretBuffer&lt;/code&gt;. The uncovered lines are there to deal with the case where the &lt;code&gt;SecretBuffer&lt;/code&gt;&#39;s data array is totally full (the &lt;code&gt;ptr&lt;/code&gt; is past its &lt;code&gt;length&lt;/code&gt;). In the paper analogy, this would mean that we wrote all the way to the bottom right corner and need to turn over to a fresh page.&lt;/p&gt;
&lt;h1 id=&#34;writing-the-test&#34;&gt;Writing the test&lt;/h1&gt;
&lt;p&gt;To test this case, what I chose to do was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a small &lt;code&gt;SecretBuffer&lt;/code&gt; and write up to its data size&lt;/li&gt;
&lt;li&gt;Write a little more data, so that it will have to call this untested code and extend &lt;code&gt;data&lt;/code&gt; in-place&lt;/li&gt;
&lt;li&gt;Make sure all the data I wrote survived the process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Julia&#39;s tests live in the &lt;code&gt;test/&lt;/code&gt; directory, which is in the top-level with &lt;code&gt;base/&lt;/code&gt; and &lt;code&gt;stdlib/&lt;/code&gt;. From my julia clone, I can see what it contains:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ls ~/projects/julia/test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you do this, you&#39;ll see there is a file called &lt;code&gt;test/secretbuffer.jl&lt;/code&gt;. That looks promising, and indeed, it&#39;s where tests for &lt;code&gt;SecretBuffer&lt;/code&gt; are (as opposed to tests for another kind of buffer, put there as a prank). Now you can open your favorite text editor and modify the file. I like to see if there is an obvious place to add the tests in the file &amp;ndash; for example, if I&#39;m testing a function for subtracting complex numbers, and there are already a bunch of tests for adding them, that would be a natural location to add my test.&lt;/p&gt;
&lt;p&gt;Julia test files tend to be organized into &lt;code&gt;testset&lt;/code&gt;s, which group similar tests and make it easier to pinpoint where the problem is when one fails. Since we&#39;re testing something specific, it makes sense to add our own little testset nested inside the main &lt;code&gt;SecretBuffers&lt;/code&gt; testset (as the others are):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;w&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# ready to test some stuff in here!&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Testsets get a name, so that if they fail we can see what&#39;s broken. It&#39;s best to write something descriptive for this rather than &amp;ldquo;aaaaa&amp;rdquo;,  &amp;ldquo;aaaab&amp;rdquo;,  etc. Following what I wrote above, I need to create a small &lt;code&gt;SecretBuffer&lt;/code&gt;&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;w&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
        sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where &lt;code&gt;sizehint&lt;/code&gt; tells Julia how big to make the initial internal &lt;code&gt;data&lt;/code&gt; array (you can see this in the type definition I copied above). Now I need to write some data to the buffer to read the end of the data array, which has two slots currently. I picked the biggest integer &lt;code&gt;UInt8&lt;/code&gt; can represent, because its bits will all be non-zero (here I am anticipating what I&#39;m going to test at the end).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;w&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
        sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# data vector will not grow&lt;/span&gt;
        bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; typemax&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now the data vector is fully saturated, and we will test the growing part of it by trying to write one more time:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;w&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
        sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# data vector will not grow&lt;/span&gt;
        bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; typemax&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# data vector must grow&lt;/span&gt;
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, it would be good to make sure this write didn&#39;t silently fail somehow or corrupt the previously written data, so we are going to read back the buffer and make sure its contents are what we expect, which is a &lt;code&gt;String&lt;/code&gt; with three identical elements:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;    &lt;span style=&#34;color:#a6e22e&#34;&gt;@testset&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;w&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;!&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;u&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;c&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;o&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;v&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;begin&lt;/span&gt;
        sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SecretBuffer&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sizehint&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# data vector will not grow&lt;/span&gt;
        bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; typemax&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        &lt;span style=&#34;color:#75715e&#34;&gt;# data vector must grow&lt;/span&gt;
        write&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, bits&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
        seek&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
        &lt;span style=&#34;color:#a6e22e&#34;&gt;@test&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; read&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, String&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\xff&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\xff&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\xff&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
        shred!&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(sb&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We &lt;code&gt;seek&lt;/code&gt; back to the start of the data to make sure we capture all three elements. We read the buffer as a &lt;code&gt;String&lt;/code&gt; and compare with what the result should be. How did I know it would be three &lt;code&gt;\xff&lt;/code&gt;?&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;julia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; String&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;([typemax&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;UInt8&lt;/span&gt;)])
&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\xff&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can make sure the test works by running it. I nearly always use the Makefile to do this, because it will catch whitespace errors, which I make prodiguously.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd ~/projects/julia
make test-secretbuffer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can run any test in &lt;code&gt;test/&lt;/code&gt; this way, for example &lt;code&gt;make test-arrayops&lt;/code&gt; works too. Assuming the tests pass, we are ready to commit and move on. If they don&#39;t pass, you&#39;ll need to look at the error message and try to figure out how to fix the test and/or the code it&#39;s supposed to be testing.&lt;/p&gt;
&lt;h1 id=&#34;commiting-and-pushing&#34;&gt;Commiting and pushing&lt;/h1&gt;
&lt;p&gt;We need to commit our changes and push them to a remote so that we can open a pull request, which will let someone review the proposed changes and (hopefully) merge them. Although it&#39;s possible to do this from your &lt;code&gt;master&lt;/code&gt; branch, this can very quickly lead to tears. It&#39;s better to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a new &lt;code&gt;git&lt;/code&gt; branch&lt;/li&gt;
&lt;li&gt;Commit the changes there&lt;/li&gt;
&lt;li&gt;Push that branch to your fork&lt;/li&gt;
&lt;li&gt;Open a pull request from the branch on your fork&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can use &lt;a href=&#34;https://www.git-scm.com/docs/git-checkout&#34;&gt;&lt;code&gt;git checkout -b&lt;/code&gt;&lt;/a&gt; to both create a new branch and move over onto it, which will drag all our changes along with. &lt;code&gt;git checkout -b&lt;/code&gt; is a shorter way of saying &lt;code&gt;git branch $BRANCH_NAME &amp;amp;&amp;amp; git checkout $BRANCH_NAME&lt;/code&gt;. You can choose whatever you like for the branch name, but I tend to prepend with my initials and then use a short description of what the changes were. As an example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git checkout -b ksh/sbtest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we&#39;re ready to &lt;a href=&#34;https://www.git-scm.com/docs/git-commit&#34;&gt;commit&lt;/a&gt;. Since the change was just to one file, and pretty small, we can do a command-line commit and bypass the editor. We&#39;ll commit and pass the &lt;code&gt;-m&lt;/code&gt; flag with an accompanying &lt;em&gt;commit message&lt;/em&gt;, which will tell everyone what the changes do.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git commit test/secretbuffer.jl -m &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Added test for write-ing past the pre-existing data length&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I told &lt;code&gt;git&lt;/code&gt; which file(s) to use in the commit and the message to attach to them. The commit will be made to the &lt;code&gt;ksh/sbtest&lt;/code&gt; branch. Now I&#39;m ready to push. Because, for the purposes of this example, I don&#39;t have write access to &lt;code&gt;JuliaLang/julia&lt;/code&gt;, I will push to my fork at &lt;code&gt;kshyatt/julia&lt;/code&gt; and open the pull request from there. All I have to do is tell &lt;code&gt;git&lt;/code&gt; to push to the &lt;code&gt;kshyatt&lt;/code&gt; remote I created at the start of this post.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;~/projects/julia $ git push kshyatt ksh/sbtest
Enumerating objects: 7, &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;.
Counting objects: 100% &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;7/7&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;.
Delta compression using up to &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; threads
Compressing objects: 100% &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;4/4&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;.
Writing objects: 100% &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;4/4&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;613&lt;/span&gt; bytes | 306.00 KiB/s, &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;.
Total &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;delta 3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, reused &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;delta 0&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
remote: Resolving deltas: 100% &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;3/3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;, completed with &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; local objects.
remote: 
remote: Create a pull request &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ksh/sbtest&amp;#39;&lt;/span&gt; on GitHub by visiting:
remote:      https://github.com/kshyatt/julia/pull/new/ksh/sbtest
remote: 
To github.com:kshyatt/julia.git
 * &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;new branch&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;            ksh/sbtest -&amp;gt; ksh/sbtest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Hooray! The push to my remote succeeded. The changes have been propagated &amp;ldquo;upstream&amp;rdquo; and can be used to open the pull request.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Important Note&lt;/em&gt;: Make sure to open the pull request from &lt;code&gt;JuliaLang/julia&lt;/code&gt;, and &lt;em&gt;not&lt;/em&gt; from your fork!&lt;/p&gt;
&lt;p&gt;To open the PR, I open up the &lt;a href=&#34;https://github.com/JuliaLang/julia&#34;&gt;main Julia repo&lt;/a&gt; and lo and behold, GitHub has a suggestion for me! I sure do want to make this pull request, so I hit the green button that says &amp;ldquo;Compare and Pull Request&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forkbutton.png&#34; alt=&#34;Comparing with Julia&#34;&gt;&lt;/p&gt;
&lt;p&gt;That leads to the comparison screen, where I can see my proposed changes and write some comments describing what they do.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;openpr.png&#34; alt=&#34;PR editing&#34;&gt;&lt;/p&gt;
&lt;p&gt;GitHub has also helpfully used my commit message to title the pull request, and I think it&#39;s quite a nice title so I will leave it be. This PR is pretty self-explanatory so I won&#39;t write a description in the big text box provided, but it doesn&#39;t hurt to do this if you think it might not be obvious what your code does. Don&#39;t worry about requesting a reviewer or adding labels if this is your first PR - a maintainer can handle that for you. If everything looks ok, press the &amp;ldquo;Create pull request&amp;rdquo; button. At this point, you can wait for some reviews and maintainers can give you detailed help if your PR needs some changes. It&#39;s very common for a PR to need some tweaks &amp;ndash; it happens to me all the time! Although it can feel a little discouraging to have to keep making changes, maintainers are devoting the time to re-reviewing your PR because they think it&#39;s good and want it to be the best it can be (just like Julia as a whole). If you want to see the real-life PR that I created in making this post, it&#39;s right &lt;a href=&#34;https://github.com/JuliaLang/julia/pull/30921&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Less than half a minute after I opened the pull request, super speedy reviewer Matt Bauman thought it was alright to merge it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;review.png&#34; alt=&#34;Matt attack&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we just had to wait for the continuous integration (CI) servers to run the tests. There are CI servers for Windows, Linux, macOS, ARM, and freeBSD. If &amp;ldquo;the lights turn green&amp;rdquo; (all tests passed), and someone has approved the PR, it should be merged very soon. If 24 hours have passed and it hasn&#39;t been merged, feel free to ping the person who approved the PR and ask them if they can merge it for you. Sometimes these things fall by the wayside. Even if CI fails, it may be an incidental failure unrelated to your changes. If it is related, someone will be happy to help you figure out what the problem is, or you can dig through the logs yourself to try to figure it out. If no one reviews your PR after a day or two, you can post a comment on the PR asking for review, or come onto Slack and bug us about it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From One To Many</title>
      <link>https://kshyatt.github.io/talk/juliacon2017/</link>
      <pubDate>Thu, 31 Jan 2019 13:25:50 -0500</pubDate>
      <guid>https://kshyatt.github.io/talk/juliacon2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Running Cluster Jobs Remotely </title>
      <link>https://kshyatt.github.io/post/sshcluster/</link>
      <pubDate>Thu, 31 Jan 2019 11:27:44 -0500</pubDate>
      <guid>https://kshyatt.github.io/post/sshcluster/</guid>
      <description>&lt;h2 id=&#34;a-journey-through-laziness&#34;&gt;A Journey Through Laziness&lt;/h2&gt;
&lt;p&gt;Quite often, I&#39;m working at my desktop in my campus office. The compute cluster on campus does not allow me to SSH directly to nodes or to spawn jobs, so usually you SSH into the head node and queue the jobs (which may be julia jobs, if I want to run some fancy parallel Julia code).&lt;/p&gt;
&lt;p&gt;This is kind of a pain, so I tried out using &lt;a href=&#34;https://github.com/JuliaParallel/ClusterManagers.jl&#34;&gt;ClusterManagers.jl&lt;/a&gt; and julia&#39;s native &lt;code&gt;addprocs&lt;/code&gt; and remote worker functionality to automate this a bit.&lt;/p&gt;
&lt;p&gt;I have key-based login set up from &lt;code&gt;desktop&lt;/code&gt; to &lt;code&gt;cluster&lt;/code&gt; (the head node), and from &lt;code&gt;cluster&lt;/code&gt; to its worker nodes.
Let&#39;s call my script &lt;code&gt;my_bad_science.jl&lt;/code&gt; (I read somewhere that just like in the code itself, your writing about the code should contain
descriptive names).&lt;/p&gt;
&lt;p&gt;First, I wanted to open a REPL on the desktop (I could make this a script too, of course). All the below examples are julia 0.5.0.
Then I opened an SSH tunnel to the cluster:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;julia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; addprocs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;([(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;c&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;l&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;u&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;h&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;o&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;m&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)], tunnel&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;, max_parallel&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, exename&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;h&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;o&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;m&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;k&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;h&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;t&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;j&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;u&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;l&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;v&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;j&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;u&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;l&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, sshflags&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;v&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;v&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I passed &lt;code&gt;-vv&lt;/code&gt; (verbose) to SSH because otherwise the Julia worker tends to time out. &lt;code&gt;exename&lt;/code&gt; is there to tell julia that it&#39;s not installed in the same place on the cluster as it is on my desktop.&lt;/p&gt;
&lt;p&gt;The output looks like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# a bunch of verbose SSH chat happens here&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# removed to protect the guilty/innocent&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;element&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;Array&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;Int64&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
 &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just like any other call to &lt;code&gt;addprocs&lt;/code&gt;. Perfect! Now, on the remote head node, which is my worker, I will pull in my script:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# my bad science script&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; ClusterManagers&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;jl&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# our cluster uses PBS so&lt;/span&gt;
ClusterManagers&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;addprocs_pbs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# now that the workers will have been loaded&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# I need to import the packages I need to do&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the cool stuff&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; Distributions&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; research_utils&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# a private package of mine&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;#fill up some parameter arrays&lt;/span&gt;
Ls&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; collect&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
w&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10.&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# disorder strength&lt;/span&gt;
d&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Uniform&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;w&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, w&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
disorder&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [rand&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(d&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, L&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; L&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; Ls&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;]

Hs&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pmap&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(makeHamiltonians&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, Ls&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;, disorder&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)   

&lt;span style=&#34;color:#75715e&#34;&gt;# some code down here to write out the&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Hamiltonians to HDF5 - it&amp;#39;s gross :(&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What this will do is spawn a 12 worker job on the cluster, do the work, and write the results &lt;em&gt;on the cluster&lt;/em&gt;. I could also have this script return something to the master worker on my desktop, if I wanted (perhaps timing information?).&lt;/p&gt;
&lt;p&gt;Finally, on my desktop REPL, all I need to do is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;julia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; rr&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;@spawnat&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; include&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;m&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;a&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;c&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;i&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;c&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;e&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;j&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;l&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)
Future&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#66d9ef&#34;&gt;Nullable&lt;/span&gt;{&lt;span style=&#34;color:#66d9ef&#34;&gt;Any&lt;/span&gt;}())

julia&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; wait&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;(rr&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;@spawnat&lt;/code&gt; returns a &lt;code&gt;Future&lt;/code&gt;, and we need to &lt;code&gt;wait&lt;/code&gt; for it to be finished. Or, we could &lt;code&gt;schedule&lt;/code&gt; it and merrily go on our way, periodically checking to see if &lt;code&gt;rr&lt;/code&gt; is finished.&lt;/p&gt;
&lt;p&gt;One could also imagine a fancier version of this, plotting data as it comes in from the cluster head node on one&#39;s desktop (so we could use something a bit prettier than &lt;code&gt;UnicodePlots.jl&lt;/code&gt;). For that you might need a &lt;code&gt;RemoteChannel&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is pretty short but it dramatically improved my lazing about, watching-job-results-come-in workflow. I hope it&#39;s useful to someone else!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantum Statistical Simulations with Julia</title>
      <link>https://kshyatt.github.io/talk/juliacon2015/</link>
      <pubDate>Thu, 31 Jan 2019 11:22:28 -0500</pubDate>
      <guid>https://kshyatt.github.io/talk/juliacon2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Writing an entanglement geometry dictionary: tensor networks, geodesy, and quenching</title>
      <link>https://kshyatt.github.io/talk/boston2018/</link>
      <pubDate>Tue, 13 Mar 2018 18:14:19 -0400</pubDate>
      <guid>https://kshyatt.github.io/talk/boston2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extracting Entanglement Geometry from Quantum States</title>
      <link>https://kshyatt.github.io/publication/disentangling/</link>
      <pubDate>Tue, 31 Jan 2017 11:01:12 -0500</pubDate>
      <guid>https://kshyatt.github.io/publication/disentangling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Many-body localization in the presence of a small bath</title>
      <link>https://kshyatt.github.io/publication/bilayer/</link>
      <pubDate>Sun, 31 Jan 2016 11:01:12 -0500</pubDate>
      <guid>https://kshyatt.github.io/publication/bilayer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Entanglement at a Two-Dimensional Quantum Critical Point: a Numerical Linked Cluster Expansion Study</title>
      <link>https://kshyatt.github.io/publication/nlce/</link>
      <pubDate>Tue, 31 Jan 2012 11:01:17 -0500</pubDate>
      <guid>https://kshyatt.github.io/publication/nlce/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
